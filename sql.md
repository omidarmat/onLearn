- [Basics of SQL](#basics-of-sql)
  - [Creating a table](#creating-a-table)
  - [Deleting (droping) a table](#deleting-droping-a-table)
  - [Inserting rows](#inserting-rows)
  - [Retrieve data](#retrieve-data)
    - [With `SELECT`](#with-select)
      - [Retrieve raw columns](#retrieve-raw-columns)
      - [Retrieve calculated columns](#retrieve-calculated-columns)
    - [Filtering rows](#filtering-rows)
      - [With `WHERE`](#with-where)
  - [Update rows](#update-rows)
  - [Delete rows](#delete-rows)
- [Working with tables](#working-with-tables)
  - [Approaching database design](#approaching-database-design)
    - [Types of relationships](#types-of-relationships)
      - [Primary keys](#primary-keys)
      - [Foreign key](#foreign-key)
        - [Example 1: One-to-One relationship](#example-1-one-to-one-relationship)
        - [Example 2: One-to-Many relationship](#example-2-one-to-many-relationship)
      - [Wrap up](#wrap-up)
  - [Primary keys](#primary-keys-1)
  - [Foreign keys](#foreign-keys)
    - [Autogenerated IDs](#autogenerated-ids)
    - [Creating the foreign keys column](#creating-the-foreign-keys-column)
      - [Retrieve by the foreign key (`JOIN`)](#retrieve-by-the-foreign-key-join)
    - [Data consistency with foreign key constraints](#data-consistency-with-foreign-key-constraints)
    - [Foreign key constraints around deletion](#foreign-key-constraints-around-deletion)
- [Relating records with joins and aggregations](#relating-records-with-joins-and-aggregations)
  - [Joins](#joins)
    - [Examples](#examples)
    - [Different kinds of joins](#different-kinds-of-joins)
      - [Inner join](#inner-join)
      - [Left outer join](#left-outer-join)
      - [Right outer join](#right-outer-join)
      - [Full join](#full-join)
    - [`WHERE` with `JOIN`](#where-with-join)
    - [Three way joins](#three-way-joins)
  - [Aggregation and grouping](#aggregation-and-grouping)
    - [Grouping](#grouping)
    - [Aggregates](#aggregates)
    - [Combining `GROUP BY` and aggregates](#combining-group-by-and-aggregates)
    - [Filtering groups with `HAVING`](#filtering-groups-with-having)
- [Basics of sorting](#basics-of-sorting)
  - [Sorting variation: number](#sorting-variation-number)
  - [Sorting variation: string](#sorting-variation-string)
  - [Sorting variation: based on multiple properties](#sorting-variation-based-on-multiple-properties)
  - [`OFFSET` and `LIMIT`](#offset-and-limit)
    - [`LIMIT` and `ORDER BY` and `OFFSET`](#limit-and-order-by-and-offset)
- [`UNION`, `INTERSECRT` and `EXCEPT` with sets](#union-intersecrt-and-except-with-sets)
  - [Some rules of unions](#some-rules-of-unions)
  - [Commonalities with intersections](#commonalities-with-intersections)
- [Assembling queries with subqueries](#assembling-queries-with-subqueries)
  - [Subqueries in `SELECT`](#subqueries-in-select)
  - [Subqueries in `FROM`](#subqueries-in-from)
  - [Subqueries in `JOIN`](#subqueries-in-join)
  - [Subqueries in `WHERE`](#subqueries-in-where)
    - [Examples](#examples-1)
  - [Correlated subquery](#correlated-subquery)
  - [`SELECT` without a `FROM` or `JOIN`](#select-without-a-from-or-join)
- [Selecting `DISTINCT` records](#selecting-distinct-records)
- [Utility operators](#utility-operators)
  - [`GREATEST` function](#greatest-function)
  - [`LEAST` function](#least-function)
  - [`CASE` keyword](#case-keyword)
- [Postgres complex data types](#postgres-complex-data-types)
  - [Data types](#data-types)
    - [Numeric types](#numeric-types)
      - [Rules on number type](#rules-on-number-type)
    - [Character types](#character-types)
    - [Boolean types](#boolean-types)
    - [Date/time types](#datetime-types)
      - [Calculations with date and time](#calculations-with-date-and-time)
- [Database-side validation and constraints](#database-side-validation-and-constraints)
  - [Row-level validation](#row-level-validation)
    - [Applying null constraint](#applying-null-constraint)
    - [Default column values](#default-column-values)
    - [Appylying unique constraint](#appylying-unique-constraint)
      - [Multi-column uniqueness](#multi-column-uniqueness)
    - [Droping constraints](#droping-constraints)
    - [Adding validation check](#adding-validation-check)
      - [Checks over multiple columns](#checks-over-multiple-columns)
- [Database structure design patterns](#database-structure-design-patterns)
  - [Building some schema](#building-some-schema)
  - [Going through a real-wold example](#going-through-a-real-wold-example)
    - [How to build a like system](#how-to-build-a-like-system)
      - [How not to design a like system](#how-not-to-design-a-like-system)
      - [Designing a like system](#designing-a-like-system)
        - [Polymorphic associations](#polymorphic-associations)
        - [Alternative design](#alternative-design)
        - [Easier alternative](#easier-alternative)
        - [Conclusion](#conclusion)
    - [How to build a mention system](#how-to-build-a-mention-system)
      - [One table for both](#one-table-for-both)
      - [Separate tables](#separate-tables)
      - [Conclusion](#conclusion-1)
    - [How to build a hashtag system](#how-to-build-a-hashtag-system)
    - [How to build a follower system](#how-to-build-a-follower-system)
- [Implementing Design in Postgres](#implementing-design-in-postgres)
  - [Creating tables with checks](#creating-tables-with-checks)
    - [Creating the users table](#creating-the-users-table)
    - [Creating the posts table](#creating-the-posts-table)
    - [Creating the comments table](#creating-the-comments-table)
    - [Creating the likes table](#creating-the-likes-table)
    - [Creating the photo tags and caption tags tables](#creating-the-photo-tags-and-caption-tags-tables)
    - [Creaing the hashtags table](#creaing-the-hashtags-table)
- [Managing the database](#managing-the-database)
  - [Inserting data from a backup file](#inserting-data-from-a-backup-file)
  - [Restoring a database if accidentally deleted](#restoring-a-database-if-accidentally-deleted)
- [Understanding the internals of Postgres](#understanding-the-internals-of-postgres)
  - [Where does Postgres stores data on hard disk](#where-does-postgres-stores-data-on-hard-disk)
    - [Heaps, blocks and tuples](#heaps-blocks-and-tuples)
      - [Heap or heap file](#heap-or-heap-file)
      - [Tuple or item](#tuple-or-item)
      - [Block or page](#block-or-page)
  - [Block data layout](#block-data-layout)
  - [Heap file layout](#heap-file-layout)
- [Indexes for performance](#indexes-for-performance)
  - [What is an index](#what-is-an-index)
  - [How an index is created](#how-an-index-is-created)
  - [Creating an actual index in Postgres](#creating-an-actual-index-in-postgres)
    - [Droping an index](#droping-an-index)
  - [Benchmarking queries](#benchmarking-queries)
  - [Downsides of indexes](#downsides-of-indexes)
  - [Types of index](#types-of-index)
  - [Automatically generated indexes](#automatically-generated-indexes)
  - [Behind the scenes of indexes](#behind-the-scenes-of-indexes)
- [Query tuning](#query-tuning)
  - [The query processing pipeline](#the-query-processing-pipeline)
    - [Parser](#parser)
    - [Rewriter](#rewriter)
    - [Planner](#planner)
    - [Executor](#executor)
  - [`EXPLAIN` and `EXPLAIN ANALYZE`](#explain-and-explain-analyze)
    - [Understanding cost analysis](#understanding-cost-analysis)
- [Common Table Expressions (CTE)](#common-table-expressions-cte)
  - [Simple table expressions](#simple-table-expressions)
  - [Recursive common table expressions](#recursive-common-table-expressions)
    - [Step 1: Define the results and working tables](#step-1-define-the-results-and-working-tables)
    - [Step 2: Run the non-recursive statement](#step-2-run-the-non-recursive-statement)
    - [Step 3: Run the recursive statement](#step-3-run-the-recursive-statement)
    - [Step 4: Append recursive statement's result to the results table and run recursion again](#step-4-append-recursive-statements-result-to-the-results-table-and-run-recursion-again)
  - [When to use recursive CTE?](#when-to-use-recursive-cte)
- [Simplify queries with views](#simplify-queries-with-views)
  - [A possible solution](#a-possible-solution)
  - [A better solution: Create a view](#a-better-solution-create-a-view)
  - [When to use a view](#when-to-use-a-view)
  - [Deleting and changing a view](#deleting-and-changing-a-view)
- [Optimizing queries with materialized view](#optimizing-queries-with-materialized-view)
  - [A slow query](#a-slow-query)
  - [Better solution](#better-solution)
- [Managing database design with schema migrations](#managing-database-design-with-schema-migrations)
  - [Migration files](#migration-files)
  - [Issues solved by migrations](#issues-solved-by-migrations)
  - [Writing migration files](#writing-migration-files)
  - [Practical example](#practical-example)
- [Schema vs. data migration](#schema-vs-data-migration)
  - [Dangers around data migrations](#dangers-around-data-migrations)
  - [Properly running data and schema migrations](#properly-running-data-and-schema-migrations)
  - [Let's run migrations in practice](#lets-run-migrations-in-practice)
    - [Running an Express server with database connection](#running-an-express-server-with-database-connection)
    - [Adding the `loc` column](#adding-the-loc-column)
    - [Updating server code](#updating-server-code)
    - [Data migration for previous data](#data-migration-for-previous-data)
      - [Implementing the data transaction](#implementing-the-data-transaction)
    - [Updating server code (again, to now only generate `loc` data)](#updating-server-code-again-to-now-only-generate-loc-data)
    - [Droping `lat` and `lng` columns](#droping-lat-and-lng-columns)
- [Accessing Postgres from APIs](#accessing-postgres-from-apis)
  - [Building the API with users router](#building-the-api-with-users-router)
    - [Creating the server in `app.js`](#creating-the-server-in-appjs)
    - [Creating the routes file](#creating-the-routes-file)
    - [Establishing connection with database](#establishing-connection-with-database)
    - [Start the Express app and using the pool](#start-the-express-app-and-using-the-pool)
    - [Adding methods to the pool](#adding-methods-to-the-pool)
- [Data access patterns](#data-access-patterns)
  - [Creating a repository](#creating-a-repository)
    - [Creating a plain object](#creating-a-plain-object)
    - [Creating a class](#creating-a-class)
    - [Creating a class with static methods](#creating-a-class-with-static-methods)
  - [Testing the API and database connection](#testing-the-api-and-database-connection)
  - [Finding particular rows (users)](#finding-particular-rows-users)
- [Security around Postgres](#security-around-postgres)
  - [SQL injection exploits](#sql-injection-exploits)
  - [Handle SQL injection with prepared statements](#handle-sql-injection-with-prepared-statements)
  - [Reminder on POST requests](#reminder-on-post-requests)
  - [Handling updates](#handling-updates)
  - [Deleting users](#deleting-users)
- [Fast parallel testing](#fast-parallel-testing)
  - [Assertions around user count](#assertions-around-user-count)
  - [Connecting to database for tests](#connecting-to-database-for-tests)
  - [Disconnecting after tests](#disconnecting-after-tests)
  - [Multi-database setup and fixing some errors](#multi-database-setup-and-fixing-some-errors)
  - [Issues with parallel tests](#issues-with-parallel-tests)
  - [Isolation with schemas](#isolation-with-schemas)
    - [How schemas work behind the scenes](#how-schemas-work-behind-the-scenes)

# Basics of SQL

## Creating a table

In order to create a table you can use this query:

```sql
CREATE TABLE cities (
    name VARCHAR(50),
    country VARCHAR(50),
    population INTEGER,
    area INTEGER
);
```

> You should be mindful of using the ',' and ';' characters in their right position.

## Deleting (droping) a table

To delete a table along with all its records you can use this command:

```sql
DROP TABLE photos;
```

## Inserting rows

In order to insert records (rows) into the table, you can use this query:

```sql
INSERT INTO cities (name, country, population, area)
VALUES ('Tokyo', 'Japan', 38505000, 8223);
```

To insert multiple records in one go you can use this query:

```sql
INSERT INTO cities (name, country, population, area)
VALUES
    ('Delhi', 'India', 28125000, 2240),
    ('Shanghai', 'China', 22125000, 4015),
    ('Sao Paulo', 'Brazil', 20935000, 3043);
```

## Retrieve data

Retrieving data from an SQL table can be very complicated, but let's stick to the basics now.

### With `SELECT`

You can retrieve raw data from the database, or you can make the database calculate some data before returning it to you.

#### Retrieve raw columns

To retrieve data using the `SELECT` statement you can use this query:

```sql
SELECT * FROM cities;
```

Where `*` means every column. But you can also specify which data columns your want to retrieve:

```sql
SELECT name, country FROM cities;
```

#### Retrieve calculated columns

To retrieve calculated columns based on the raw data that exists in the table, you can use this query:

```sql
SELECT name, population / area FROM cities;
```

You will see in the results that the caluclated column has a title of `?column?`. You can rename the calculated column using this query:

```sql
SELECT name, population / area AS population_density
FROM cities;
```

> You can also use other math **operators** like `+`, `-`, `\*`, `/`, `^`, `|/` (square root), `@` (absolute value), `%` (remainder).

To manipulate string columns before being returned, you can use string operators or functions. For instance:

```sql
SELECT name || ', ' || country FROM cities;
```

> Note that you can use string constants (`', '` in the example above ) when manipulating string columns using string operators.

> There are also string **operators** and **functions** that you can use to manipulate string columns. These are `||` (join two strings), `CONCAT()` (join two strings), `LOWER()` (gives a lowercase string), `LENGTH()` (gives number of characters in a string), `UPPER()` (gives an uppercase string).

To use the `CONCAT()` string function to receive the exact result of the example above, you can use this query:

```sql
SELECT CONCAT(name, ', ', country) AS location
FROM cities;
```

You can stack as many string functions as you need in your query. For instance:

```sql
SELECT
  CONCAT(UPPER(name), ', ', UPPER(country)) AS location
FROM
  cities;
```

As another example:

```sql
SELECT
  UPPER(CONCAT(name, ', ', country)) AS location
FROM
  cities;
```

### Filtering rows

#### With `WHERE`

To filter rows, you can use the `WHERE` statement after `FROM`, which is where you define which table you are retrieving data from.

```sql
SELECT name, area FROM cities WHERE area > 4000
```

You may need to understand how the three parts of this query is executed by SQL in order to fully get how it works. So it just does not start from the beginning and proceed to the end. It starts from the `FROM` statement, then goes to the `WHERE` statement, and finally to the `SELECT` statement. Understanding this will help you write more complicated queries.

> Note that the query syntax formatting of the example above can also be like this:

```sql
SELECT
  name,
  area
FROM
  cities
WHERE
  area > 4000
```

> To filter rows you can use many comparison math operators like `=`, `>`, `<`, `>=`, `<=`, `IN` (is the value present in a list?), `<>` (are the values not equal), `!=`, `BETWEEN` (is the value between two other values?), `NOT IN` (is the value not present in a list?).

Let's use the `BETWEEN` operator:

```sql
SELECT
  name,
  area
FROM
  cities
WHERE
  area BETWEEN 2000 AND 4000;
```

Let's use the `IN` and `NOT IN` operator:

```sql
SELECT
  name,
  area
FROM
  cities
WHERE
  name IN ('Delhi', 'Shanghai');
```

> Note that you are not limited to use strings with the `IN` and `NOT IN` operator.

```sql
SELECT
  name,
  area
FROM
  cities
WHERE
  area NOT IN (8223, 3043);
```

> You can use as many operators in one `WHERE` statement. This is called a _Compound check_ where you can use as many `AND` and `OR` statements. Here is an example:

```sql
SELECT
  name,
  area
FROM
  cities
WHERE
  area NOT IN (8223, 3043)
  OR name = 'Delhi'
  OR name = 'Tokyo';
```

In this query you want to find all the cities that does not have an area of `8223` or `3043`, `AND` also the city must have the name `Delhi`.

> You can perform calculations in the `WHERE` statement. You just need to keep in mind that the order of execution in the filtering statement is prioritised for mathematical calculations and then comparisons. In the example below, `population` will be divided by `area` and then the result will be compared to `6000`.

```sql
SELECT
	name,
  population / area AS population_density
FROM
	cities
WHERE
	population / area > 6000
```

> Remember that you cannot refer to the renamed calculated columns with their provided name in the `WHERE` statement. You can only refer to them by doing the calculation again in the `WHERE` statement.

## Update rows

To update a row in a table, you should use the `UPDATE` statement with the table name, and then the `SET` statement with the updating property along with its new value. Finally you have to specify which row you want to update by filtering the rows using the `WHERE` statement. So as an example:

```sql
UPDATE
  cities
SET
  population = 39505000
WHERE
  name = 'Tokyo';
```

> Remember that if you are trying to update only one specific row, the `WHERE` statement you write should be specific enough to target only one record in the table. This can become a bit tricky sometimes. There is a nice solution for this problem. [later...]

## Delete rows

To delete a specific row from a table, you should use the `DELETE FROM` statement followed by the table name, and then the `WHERE` statement that defines the row that is going to be deleted.

```sql
DELETE FROM cities
WHERE name = 'Tokyo';
```

> Remember that if you are trying to update only one specific row, the `WHERE` statement you write should be specific enough to target only one record in the table. This can become a bit tricky sometimes. There is a nice solution for this problem. [later...]

# Working with tables

Things are going to get closer to real-world conditions now. We are going to work with multiple tables in one database. In this part of this tutorial we are going to design a database for a photo-sharing application.

## Approaching database design

Let's talk about a few tips that would help you come up with your own database design that would suit your application needs.

The most important question you want to ask yourself is: **What Tables should we make?**

A lot of time for any application you make, chances are it is going to have many features that are common among many other applications. There are many features out in the world (e.g. authentication, liking system, commenting systems) that many different web apps implement. Therefore, there are tons of resources online to give you suggestions on how to structure your database to implement these features.

However, you are probably going to create an app with some features that no one has ever built. To address this case, you need to take a look at some mockups of your application to see what the user interface looks like and what is the purpose of your application. Out goal here is to identify the different kinds of resources that exists inside your app. So you need to ask yourself: **What type of resources exist in your app?** Then as the first step, you might want to create a separate table for each of these features or resources.

Then for each of these resources, you might want to find relationship or ownership between two types of resources. These relationships must be reflected in the database design. This is only achievable in a practical example.

Take Instagram as an example. To design the database for Instagram, we need 4 tables: users, photos, comments, likes. We also know that each user can own comments, photos and likes. We also know that photos own comments and likes by users. These are the relationships.

To represent relationships, we need some data somewhere that says, for instnace, that some specific photos belong to a specific user. We should now learn about 4 different kinds of relationships.

### Types of relationships

1. One-to-Many: for example, a user can own many photos. Also, a photo can have many different comments tied to it.
2. One-to-One: for example, a boat can only have one captain. A company can only have one CEO. A CEO can relate to only one company at a time. A student has one desk, and a desk can only belong to one student.
3. Many-to-Many: this is a bit trickier. For example, many students may be related to many classes. Many tasks may be assigned to many engineers. Many movies have many actors in them. Multiple sessions of conference calls can involve multiple employees.
4. Many-to-One: it is kind of the opposite direction of One-to-Many relationship. Many different comments can belong to one photo.

#### Primary keys

Primary keys are additional columns that you add to a lot of different tables inside all the different database your create. Every single table you make will have a primary key. The goal of a primary key is to identify an individual row inside a table. Every value inside the primary key volumn is going to be some kind of a unique value in that column. It is basically the ID of each row.

#### Foreign key

To set up a relationship between two different records, you should use a foreign key. The goal of a foreign key is to relate a record in a table, to another record in possibly another table or even in the same table.

##### Example 1: One-to-One relationship

For instance, imagine we have a table of photos and users. So to represent the relationship between a user and a photo, we should insert a foreign key of `user_id` for each record (row) in the photos table. This way, each photo will be related to a specific user. This way you would be able to query for all the photos related to a specific user (for instance, user with ID of 4).

##### Example 2: One-to-Many relationship

For instance, imaine a system like Instagram where comments are related to users and also comments are relate to a photos. Comments have one user and comments have one photo. Looking at the oposite direction, we would say that a user has many comments, and a photo has many comments. We now have to decide which table will receive the foreign keys. Here is the key: **The many side of our relationship is always going to get the foreign key column**. So now that we know a photo can have many comments, the many side is the comments. So the comments table will get the foreign keys pointing at a photo.

#### Wrap up

## Primary keys

1. Each row in every table has one primary key
2. No other row in the same table can have the same value
3. 99% of the time is called 'id'
4. It would be either an integer or a UUID
5. It will never change

## Foreign keys

1. Rows only have this if they belong to another record
2. Many rows in the same table can have the same foreign key
3. Name varies, usually called something like 'xyz_id'
4. Exactly equal to the primary key of the referenced row
5. It will change if the relationship changes

### Autogenerated IDs

When we implement a table that should have a primary key column, we can use a Postres feature which automatically creates a random ID when a new record is inserted into the row. To do this, we create the table with this syntax:

```sql
CREATE TABLE users (
  id SERIAL PRIMARY KEY,
  username VARCHAR(50)
);
```

So `SERIAL` data type tells Postgres that we want it to generated values for this column automatically. This way, when we want to insert a new user into this table, we won't have to give it an ID. The ID will be generated automatically.

Let's now add some data to this table:

```sql
INSERT INTO users (username)
VALUES
	('monahan93'),
  ('pfeffer'),
  ('si93onis'),
  ('99stroman');
```

Now if you use this command to take a look at all your records in this table:

```sql
SELECT * FROM users
```

You will see that each row has a unique ID which is number. It starts from 1 for the first user to 4 for the last.

### Creating the foreign keys column

Following the previous example, we are now going to create a photos table where we are going to store a foreign key of `user_id` for each photo record.

```sql
CREATE TABLE photos (
  id SERIAL PRIMARY KEY,
  url VARCHAR(200),
  user_id INTEGER REFERENCES users(id)
  );
```

Note that the foregin key column is define as the `user_id`, which holds the `INTEGER` value type, which `REFERENCES` to the `id` column of the `users` table. Marking a column as foreign key enforces some level of data consistency.

Let's now insert rows into this table.

```sql
INSERT INTO photos(url, user_id)
VALUES
	('http://two.jpg', 1),
  ('http://three.jpg', 1),
  ('http://four.jpg', 1),
  ('http://five.jpg', 2),
  ('http://six.jpg', 3),
  ('http://six.jpg', 4);
```

We can now see that in the record of this photo in the table, we have a `user_id` of `4`. But how is this going to help us with the relationship? There two big advantages here:

1. We can start to write a lot of interesting queries that allow us to fetch all the different photos that are associated with a user.
2. We also start to get a little bit of **data consistency**.

#### Retrieve by the foreign key (`JOIN`)

Let's now write some query that uses the foreign key that we implemented.

To fetch all the photos related to a user with id of `4`. We can do it in a simple way.

```sql
SELECT
  *
FROM
  photos
WHERE
  user_id = 4;
```

But let's now actually utilize the foreign key of each photo, and list the photos along with the username that owns the photo. So we want to receive a list of photos where, instead of some integers as `user_id`s, actual usernames are listed. So what we want basically is to get images from the photos table and usernames from the users table and get them in one new table as a result of a query. This means that we need to somehow `JOIN` the two tables. We will explain more about this complicated syntax later. [later...]

```sql
SELECT url, username FROM photos
JOIN users ON users.id = photos.user_id;
```

### Data consistency with foreign key constraints

Data consistency refers to the ability of our database to make sure that the information we are inserting into it and working with it actually makes sense, and that all the references between tables actually line up. When we insert a photo record into the photos table, there are 3 possibilities that we need to consider:

- We insert a photo that is tied to a user that exists: everything is fine.
- We insert a photo that refers to a user that does not exist: this does not make sense. We are going to end up with an error. So Postgres automatically validates the reference to make sure the `user_id` matches with an actual user on the users table. If there is no match, Postgres will respond with error.
- We insert a photo that is not tied to any user: this is when we insert a photo into the photos table and don't provide a `user_id`. Well actually, you should not leave the field empty, but you have to put `NULL` instead. Postgres will accept this insertion. This photo record will also show up in the records if we retrieve all records `*`.

### Foreign key constraints around deletion

What happens we you want to delete a record that involves a foreign key? Imagine there are 3 photos related to user `1` inside the photos table. What happens if you delete user `1`? We would then end up with some dangling references. Now those photos refer to a user that does not exist and will never exist in future (because the ID integers are serialized).

In order to prevent this situation, when we make use of foreign keys, we can specify some options or exactly what we want to have happen when we want to delete a record that some other rows in some other table are dependant upon. Here are some options available:

1. `ON DELETE RESTRIC`: This is the default option. When you try to delete a user to which a photo is referencing, Postgres will throw an error.
2. `ON DELETE NO ACTION`: This option also throws an error but with a small difference [later...]
3. `ON DELETE CASCADE`: This will delete the user and also delete the photos related to that user.
4. `ON DELETE SET NULL`: This will set the `user_id` on the related photos to `NULL`. So the photos will remain in the database but will no longer refer to any user.
5. `ON DELETE SET DEFAULT`: This will set the `user_id` of the photo to a default value if this default value is provided. This default value should be determined at the time of creating the table.

Let's explore how we can use these options. To use the `ON DELETE CASCADE` you can use this syntax:

```sql
CREATE TABLE photos (
  id SERIAL PRIMARY KEY,
  url VARCHAR(200),
  user_id INTEGER REFERENCES users(id) ON DELETE CASCADE
);
```

Using this option is pretty common in managing the database of a forum application. Usually, when a user is deleted, their comments, posts and other related things are deleted also.

Let's now see how we use `ON DETELE SET NULL`:

```sql
CREATE TABLE photos (
  id SERIAL PRIMARY KEY,
  url VARCHAR(200),
  user_id INTEGER REFERENCES users(id) ON DELETE SET NULL
);
```

Using this option is recommended when you want to delete a user but keep the resources related to them.

# Relating records with joins and aggregations

In this part we are going to deal with some more complex queries. We are going to implement them using two important techniques: Joins and Aggregation.

## Joins

We use joins in SQL queries to produce a value or a set of rows by merging together a lot of different rows from different tables. We usually use joins anytime you are trying to answer a question related to multiple different kinds of resources. For instance, if you want to solve this question:

Find all the comments for the photo with ID `3` along with the username of the comment author.

This question is mentioning comments, photos, and users. Since you are going to deal with multiple resources, it might be a sign that you are going to need to use joins.

### Examples

As the first example we are going to answer this question: For each comment, show the contents of the comment and the username of the user who wrote the comment. This is definitely where you want to use joins.

```sql
SELECT contents, username
FROM comments
JOIN users ON users.id = comments.user_id;
```

We are now going to throughly examin what actually the `JOIN` command does. First, we know that we need some information from both comments and users. This is a sign that we need to use joins. Second, we are trying to refer to some columns or information in both tables; contents in comments, and usernames in users. This is another sign that we need to use a join.

Now about the syntax above, we start with the `FROM` statement. This is basically an initial selection of the rows that we want to operate on. This is actually taking all the rows from comments. Next comes the `JOIN` statement. This statements determines from which table we are going to join information with the table that we stated in the `FROM` statement. So we want to join users with comments. You can think of the `JOIN` statement as making a copy of each of the tables and put them next to each other, but by matching the references together in this new table. Then from this new table, we will go to the `SELECT` statement, where we determine which columns we want to be displayed finally in the database response.

Let's now solve another question: For each comment, list the contents of the comment and the URL of the photo the comment was added to.

```sql
SELECT contents, url
FROM comments
JOIN photos ON photos.id = comments.photo_id;
```

> Note that changing the order of stating tables in a join might sometimes result in different results, while other times it might work just fine and will give you the same result. The example above is one of those situations where changing the order does not matter. So the syntax below will give you the same result. We will see in which situations it will not work like this. The order matters if we are doing a [left outer join](#left-outer-join), or [right outer join](#right-outer-join).

```sql
SELECT contents, url
FROM photos
JOIN comments ON photos.id = comments.photo_id;
```

> Note that if we have two column names in the joined table that are the same, we will have to provide something called context. In this case, if we query that column name, Postgres will respond with an error saying that the column name is ambiguous. To solve this issue we should provide a context for this column name. The context is actually the name of the table from which we are targetting the column name. You insert the name with a dot right before the column name. For instance:

```sql
SELECT photos.id
FROM photos
JOIN comments ON photos.id = comments.photo_id;
```

You can also rename the column using this syntax:

```sql
SELECT photos.id AS photo_id
FROM photos
JOIN comments ON photos.id = comments.photo_id;
```

> Tables can also be renamed using the `AS` keyword. This is useful in more complex queries. After renaming the table, anywhere in the query that you are exactly referring to that table, you can use the new name you gave it.

```sql
SELECT p.id AS photo_id
FROM photos AS p
JOIN comments ON p.id = comments.photo_id;
```

> The `AS` keyword in the table renaming syntax can simply be dropped off, and the query will work just fine. But try not to do these things in your queries. Keep them readable.

### Different kinds of joins

The queries we make by **joining** tables work fine if all the records in the queried table has a **reference** to a record in another table. However, if there is a record in the queried table that does not reference any record in the other table, and we want the query to list all the items in the queried table, we would have to account for that special record with no reference. Otherwise, it won't show up.

This is a situation that can happen many times. Imagine you insert an image into the photos table with no reference in the `user_id` column.

```sql
INSERT INTO
  photos (url, user_id)
VALUES
  ('https://banner.jpg', NULL)
```

We are now going to query for all the records in the photos table by joinging in the users table.

```sql
SELECT url, username
FROM photos
JOIN users ON users.id = photos.user_id
```

The response returned by Postgres will not include the recent record that we inserted. But what if we wanted to query for all the photos so that we could analyze the whole memory amount occupied by them? We need to somehow include that photo in the result of our query.

Currently, the join logic will only list photos that satisfy the join condition. To go around this limit, we need to learn about different kinds of joins.

#### Inner join

Photos table | Users table

Whenever you use the `JOIN` statement by itself in a query, that is by default an inner join. Alternatively, you can write `INNER JOIN` to explicitely indicate that you are doing an inner join.

Inner join joins two tables together. When there is a record in one table that has no reference to a record in the other table, that row is going to be dropped out of the result set.

```sql
SELECT url, username
FROM photos
JOIN users ON users.id = photos.user_id;
```

#### Left outer join

Photos table | Users table

Following the previous example, if there are any records in the photos table that does not reference to any record in the users table, those records in the photos table will not be dropped out of a query that queries for all the photos.

```sql
SELECT url, username
FROM photos
LEFT JOIN users ON users.id = photos.user_id;
```

> Changing the order of performing the query in left outer join will affect the result.

#### Right outer join

Photos table | Users table

If there are any records in the photos table that have no reference to any record in the users table, those records will be dropped out of a query that queries for all the photos. However, all the records from the users table will be included in the result set, including those that are not referenced by any of the photos.

```sql
SELECT url, username
FROM photos
RIGHT JOIN users ON users.id = photos.user_id;
```

> Changing the order of performing the query in right outer join will affect the result.

#### Full join

Photos table | USers table

This join tells that just give me everything and I dont' care whether or not there is any correct merging going on, just try to merge as many rows as possible and if you can't just include the rest of the records in the result set.

```sql
SELECT url, username
FROM photos
FULL JOIN users ON users.id = photos.user_id;
```

### `WHERE` with `JOIN`

Imagine you are going to write a query to answer this question: Users can comment on photos that they posted. List the url and contents for every photo/comment where this occured. This is easily solved by using a joins along with the `WHERE` statement. This means that you should basically try to find photos and comments that have the same `user_id`, which means users that have commented under their own photos.

The way to go is to first join the comments and photos tables where the comment's `photo_id` equals the `id`s in the photos table. This will give us the comments that are posted under some specific photos. Now in this joined table, we would want to find records where the `user_id` in both comments and photos are the same.

Here is the query:

```sql
SELECT
  url,
  contents
FROM
  comments
JOIN photos ON photos.id = comments.photo_id
WHERE
  comments.user_id = photos.user_id;
```

As you can expect, the result set will include the photo URLs and the comment contents. We cannot see who actually are these users who have posted comments under their own photos. What should we do to retrieve their usernames? We currently have no sign of the users table in this query.

### Three way joins

Three way joins are the solution to the problem that we mentioned previously. Conceptually, this is very difficult to understand, but the query syntax ends up being very straight forward.

What we should do is to first, get the three tables into the temporary, imaginary joind table. This table has the columns from all the three tables. We can first start with the comments table, and then go on to the photos table and join where `comments.photo_id` equals `photos.id`. Then we go on to the users table and join where `comments.user_id` equals `photos.user_id` and also equals `user.id`. Certainly, not all the rows in the joind table would match this triple equal condition. So the related user row in this joind table would be filled with `NULL` if the condition is not satisfied. But if it is satisfied, the user id and username will be inserted into the joind table.

So we are going to update the query like this:

```sql
SELECT
  url,
  contents,
  username
FROM
  comments
 JOIN photos ON photos.id = comments.photo_id
 JOIN users ON users.id = comments.user_id
  AND photos.user_id = users.id
```

## Aggregation and grouping

With all the queries you have written so far you have just essentially pulled out some set of rows, which was more or less the exact information passed into the database. However, as we move towards more complicated queries, we are going to introduce two additional techniques: groupings and aggregation.

The goal with these two techniques is to take a big set of values or rows and somehow condense them down to a smaller set of values. It would be challenging to select data or select columns out of the grouped information. To avoid this challenge, it is key to visualize what happens when you apply the group by keyword.

### Grouping

With grouping, we are going to reduce many rows down to fewer rows. Grouping is done using the `GROUP BY` keyword. For example, take the example query below and let's understand what it does:

```sql
SELECT user_id
FROM comments
GROUP BY user_id;
```

The `GROUP BY` keyword, your database will take a look all the rows that you have selected. Here we have selected all the rows of the comments table. It will now try to find all the unique values for `user_id` and it will create a separate row in an imaginary table. The database will then take each of the rows from the selected table and assign it to a different group row based upon that row's `user_id`.

It is extremely important to remember that in this imaginary grouped table, you can only select the `user_id` column which is the `GROUP BY` argument in the query above. If you want to select columns related to the comments table inside the new imaginary grouped table, you must use aggregate functions.

### Aggregates

The goal of aggregation is to take a set of rows and somehow calculate a single value out of them. Calculating 'most', 'least', 'greatest', 'average', etc. you are probably going to need to use aggragation. This is done by using the **aggregate functions**.

Here are some different aggregate functions that we can use:

1. `COUNT()`
2. `SUM()`
3. `AVG()`
4. `MIN()`
5. `MAX()`

Let's go through some examples with aggregate functions. Take the query below as an example:

```sql
SELECT MAX(id)
FROM comments;
```

This will simply return the commet record with the maxium value for the `id` comlumn. You can use any other aggregate function and see the results. You can understand the all these functions actually take a big column of values and performs some kind of calculation or math operation on all those different values and return just one single value.

Remember that when using an aggregate function, we cannot do a normal select next to it. For instance, you cannot `SUM(id)` and `SELECT id` at the same time in a query:

```sql
-- This is NOT OK
SELECT SUM(id), id
FROM comments;
```

So we are going to most frequently use these aggregate functions by themselves or as a part of a larger `GROUP BY` statement.

### Combining `GROUP BY` and aggregates

Back to example mentioned previously, how can you use aggregates to select columns in the grouped table? We can now add aggregate functions on top of grouping. An aggregate function will be applied to the individual sub-groups. If you use an aggregate function over the grouped table, you can then select the column related to that aggregate function.

Take the example below. You can see that columns of the original comments table can only be selected within an aggregate function. They can only be called to become the subject of an aggregate function.

```sql
SELECT user_id, COUNT(id)
FROM comments
GROUP BY user_id;
```

This specific query above is actually grouping all the comments from the comments table based on the `uder_id`. So each row in the grouped table includes all the comments of a specific user. So when you apply the `COUNT(id)` aggregate function, you are actually asking the database to count the number of comments created by each individual user.

> You can also rename the column created by an aggregate function. Following the example above:

```sql
SELECT user_id, COUNT(id) AS num_comments_created
FROM comments
GROUP BY user_id;
```

> A corner case about the `COUNT` aggregate function: the count function does not count if the given argument is `NULL`. For instance, if we count the rows in the photos table by the `user_id` column, only the photo rows that have an actualy user ID will be counted. If there is a photo with the `user_id` set to `NULL`, the `COUNT` aggregate function will not count it. To go around this limit you can pass `*` to the `COUNT` function. This is true in almost any case that we are using the `COUNT` function.

```sql
SELECT COUNT(*) FROM photos;
```

As another example, we are going to find the number of comments for each photo. This is the query to write:

```sql
SELECT photo_id, COUNT(*)
FROM comments
GROUP BY photo_id
```

As another more complicated example, imagine you have two tables, one for books and one for authors. You now want to write a query that will print an author's name and the number of books they have authored. Here is the solution:

```sql
SELECT name, COUNT(*)
FROM books
JOIN authors ON authors.id = books.author_id
GROUP BY authors.name;
-- also possitble to group by writing:
GROUP BY name;
```

### Filtering groups with `HAVING`

The `HAVING` keyword is used to filter the set of groups. It is pretty similar to `WHERE`. The difference however, is that `HAVING` is used to filter out some number of groups, while `WHERE` is used to filter out some number of rows. So you will never see `HAVING` without a `GROUP BY` statement.

Let's work with an example. We divide the question into separate parts:

1. Find the number of comments for each photo
2. Where the `photo_id` is less than 3: this is concerned with a value inside a particular row. This is `WHERE`.
3. And the photo has more than 2 comments: this is concerned with an aggregate function and filtering. This is about `HAVING`. Note that `HAVING` is going to have an `AGGREGATE` keyword inside it.

```sql
 SELECT photo_id, COUNT(*)
 FROM comments
 WHERE photo_id < 3
 GROUP BY photo_id
 HAVING COUNT(*) > 2;
```

As a more complicated example, let's do this: Find the users where the user has commented on the first 2 photos and the user added more than 2 comments on those photos. Let's first divide this question into separate parts:

1. Find the users IDs
2. Where the user has commented on the first 2 photos (photos with ID 1 or 2): this is `WHERE`. We are looking for comments that have the `photo_id` of 1 or 2.
3. And the user has added more than or equal to 2 comments on those photos: this is aggregation and filtering, which means `HAVING`.

```sql
SELECT user_id, COUNT(*)
FROM comments
WHERE photo_id < 3
GROUP BY user_id
HAVING COUNT(*) >= 2
```

> Most of the times, especially in more complicated queries, it is easier to leave the `SELECT` statement empty, write the other parts of the query, and then based upon the result you are expecting, fill in the `SELECT` statement.

Let's go for another example: Given a table of phones, print the names of manufacturers and total revenue (price \* units_sold) for all phones. Only print the manufacturers who have revenue greater than 2,000,000 for all the phones they sold.

```sql
SELECT manufacturer, SUM(price*units_sold)
FROM phones
GROUP BY manufacturer
HAVING SUM(price * units_sold) > 2000000;
```

# Basics of sorting

Sorting means to retrieve a number of rows from a table and then try to re-order those records based on the values of one of the columns.

## Sorting variation: number

For instance, if you want to sort a list of products sorted from the least expensive to the most expensive (ascending), you can use the `ORDER BY` statement:

```sql
SELECT *
FROM products
ORDER BY price;
```

> The default behavior of sorting is to sort data in an ascending format. Nevertheless, you can also explicitely use the `ASC` keyword to clarify the sorting behavior.

```sql
SELECT *
FROM products
ORDER BY price ASC;
```

If you want to sort from highest to lowest price, you can use the `DESC` statement at the end of the `ORDER BY` statement.

```sql
SELECT *
FROM products
ORDER BY price DESC;
```

## Sorting variation: string

You can sort the retrieved data from a table based on string values of a column using the same syntax:

```sql
SELECT *
FROM products
ORDER BY name;
```

> Keywords `ASC` and `DESC` works just like before.

## Sorting variation: based on multiple properties

Imagine you are sorting the products list by price, and then you see multiple products with the same price, and you want these products to be sorted based on some other property. So you have to sort the records based on more than just one property.

To do this you can simply state another column in the `ORDER BY` statement.

```sql
SELECT *
FROM products
ORDER BY price, weight;
```

Now if two products happen to have the same price, they will be sorted ascendingly based on their weight value.

> `ASC` and `DESC` keywords works just like before, but you would have to insert them for each column name that you have mentioned in the `ORDER BY` statement. For instance:

```sql
SELECT *
FROM products
ORDER BY price, weight DESC;
```

This means that if two products have the same price, they will be sorted descendingly regarding their weight values.

## `OFFSET` and `LIMIT`

You use `OFFSET` anytime you want to skip some number of records in a result set. For example, if you retrieve all rows from the users table and you know that there are 50 records, You can use this keyword to make Postgres skip the first 40 rows, and give you the last 10 records.

```sql
SELECT *
FROM users
OFFSET 40;
```

You use `LIMIT` when you want to constrain the number of records you get back from the result of the query. You are very likely to use `LIMIT` and `OFFSET` together. However, `LIMIT` can be used by itself.

```sql
SELECT *
FROM users
LIMIT 5;
```

> The limit value can be greater than the number of records that exist in a table.

### `LIMIT` and `ORDER BY` and `OFFSET`

As an example, you now want to order all your products by price, and then you want to retrieve the 5 least expensive products:

```sql
SELECT *
FROM products
ORDER BY price
LIMIT 5;
```

Taking this example one step further, you now want to get the 5 most expensive products except the top most expensive product:

```sql
SELECT *
FROM products
ORDER BY price DESC
LIMIT 5
OFFSET 1;
```

> Note that when using `LIMIT` and `OFFSET` together, it is a convention to insert `OFFSET` after `LIMIT`.

> `LIMIT` and `ORDER BY` are usually used in scenarios where you want to find the tops. For instance, the top five or anything like that.

> `LIMIT` and `OFFSET` are used in scenarios where you want to return results based on some pagination system in a table. So for example, if you want to display all the products to the user, you do it in a paginated format, which will present 20 products per page. This means that for the first page, you will have to return the first 20 products:

```sql
SELECT *
FROM products
ORDER BY price DESC
LIMIT 20
OFFSET 0;
```

Moving to the next page, you would have to skip the first 20 rows, and go for the second 20 rows:

```sql
SELECT *
FROM products
ORDER BY price DESC
LIMIT 20
OFFSET 20;
```

# `UNION`, `INTERSECRT` and `EXCEPT` with sets

Let's consider a realistic example: Find the 4 products with the highest price **and** the 4 products with the heighest price/weight ratio. The query for the first and second part are very different and putting them in one query is really challenging.

Let's first write the query for the first part:

```sql
SELECT *
FROM products
ORDER BY price DESC
LIMIT 4;
```

Let's now write the query for the second part:

```sql
SELECT *
FROM products
ORDER BY price / weight DESC
LIMIT 4;
```

Now to join the two queries together in order to receive one result set we can use the `UNION` keyword as below:

```sql
(
SELECT *
FROM products
ORDER BY price DESC
LIMIT 4
)
UNION
(
SELECT *
FROM products
ORDER BY price / weight DESC
LIMIT 4
);
```

> Some of the records might satisfy both queries. These records will be displayed once in the final result set. If you want those records to appear each time they satisfy a query, you can use the `UNION ALL` keyword.

```sql
(
SELECT *
FROM products
ORDER BY price DESC
LIMIT 4
)
UNION ALL
(
SELECT *
FROM products
ORDER BY price / weight DESC
LIMIT 4
);
```

## Some rules of unions

1. It is not necessary to separate the different queries using `( )`. In this specific example, we used `( )` because otherwise the database might be confused about applying the `ORDER BY` or `LIMIT` statement to just the second query or to the whole query. A union without the `( )` can look like:

```sql
SELECT * FROM products
UNION
SELECT * FROM products;
```

2. Two queries can be joined with the `UNION` keyword only if the result set of both queries have the same columns and the data type inside them must be the same also. For example, these queries cannot be unioned:

```sql
SELECT name FROM products
UNION
SELECT price FROM products;
```

It won't also work if you rename the `price` column to `name` because data types will still not be compatible.

## Commonalities with intersections

Here is a list of related keywords that are used to implement unions and intersections:

1. `UNION`: join together the result of two queries and remove duplicate rows
2. `UNION ALL`: join together results of two queries
3. `INTERSECT`: find the rows common in the results of two queries. Remove duplicates.
4. `INTERSECT ALL`: Find the rows common in the results of two queries.
5. `EXCEPT`: Find the rows that are present in first query but not second query. Remove duplicates.
6. `EXCEPT ALL`: Find the rows that are present in first query but not second query.

> Changing the order of the queries when using `UNION` and `INTERSECT` does not change the result. However, with `EXCEPT` the result will change based on the order of queries.

# Assembling queries with subqueries

To understand why we actually need subqueries let's start with an example. You have a table that includes all the products of some hyper market. What would you do if you were to write this query: List the name and price of all products that are more expensive than all products in the Toys department.

You should first try to find all the products related to the Toys department and then find the highest price among them. Finally you woul have to find the products that are more expensive than that.

As for the first stage, you cannot simply go through all the table rows to find out about the toy products. You might have a table with thousands of rows. You should do this using a query. So to write the query above, you should actually do it in 2 different steps: First, find the most expensive product in the toys department. Then, you would use the result of the first step in a second query where you will list all the products that have a price higher than the result of the first step. In this case, you can combine these two steps into one by using a subquery.

Let's first see how we would write the two steps separately:

```sql
-- FIRST STEP
SELECT MAX(price)
FROM products
WHERE department = 'Toys';

-- SECOND STEP
SELECT name, price
FROM products
WHERE price > (result-of-first-step);
```

Now to combine these two queries into one using a subquery, we would first write the outer query, or essentially the second one and insert the first query as a subquery where we need the actual result of the first query.

```sql
SELECT name, price
FROM products
WHERE price > (
  -- THIS IS WHERE THE FIRST QUERY GOES: SUBQUERY
  );
```

Now you can put any other queries inside the `( )`.

```sql
SELECT name, price
FROM products
WHERE price > (
  SELECT MAX(price)
  FROM products
  WHERE department = 'Toys'
  );
```

Understanding subqueries can be difficult since they can be used in many places in another query. But why is it hard? Because anywhere you use a subquery, you have to change the shape or type of its returned data so that it would be usable in its outer query. Some queries might produce a single value, while others will return some rows. Some of them might even produce a column. Therefore, understanding the **shape** of query result is key.

Take these queries as examples:

```sql
SELECT * FROM orders
-- Returns many rows and many columns

SELECT id FROM orders
-- Returns many rows but one column

SELECT COUNT(*) FROM orders
-- Returns one row and one column (single value) - this is also called a scalar query.
```

## Subqueries in `SELECT`

If you are going to insert a subquery into a `SELECT` statement, your subquery would have to provide a single value. So any subquery that results in a single value can be inserted into the `SELECT` statement.

```sql
SELECT name, price, (subquery-result)
```

As an example:

```sql
SELECT name, price, (
  SELECT MAX(price) FROM products
)
FROM products
WHERE price > 867
  -- This query clearly does not provide a meaningful result. It is just an example to review the rules of subqueries.
```

## Subqueries in `FROM`

When using subqueries in a `FROM` statement, you can return a wide variety of different structure of data. It is all about making sure that the outer query is compatible with the subquery.

Take this query as an example:

```sql
SELECT
  name,
  price / weight AS price_weight_ratio
FROM
  products;
```

This will return a table with two columns: name and price_weight_ratio. Now think about this query as being a subquery for another query like this:

```sql
SELECT name, price_weight_ratio
-- name and price_weight_ratio are selected based on what the subquery returns
FROM (subquery-goes-here) AS p
-- Subqueries used in FROM statements must be followed by an alias, so you would be able to refer to them with the alias
WHERE price_weight_ratio > 5
-- This is a pretty simple and easy-to-understand example which could easily be written without the subquery in a real-world scenario.
```

So the final query will be like:

```sql
SELECT
  name,
  price_weight_ratio
FROM
  (
    SELECT
      name,
      price / weight AS price_weight_ratio
    FROM
      products
  ) AS p
WHERE
  price_weight_ratio > 5
```

But why would we want to use a subquery inside a `FROM` statement? To find out about it, let's go over this example: Find the average number of orders for all users. What we basically need to do is to calculate this: total number of orders divided by total number of users.

To do this, we can first group the orders table by user id. This way we would know how many orders each user has. We can then use this as a subquery for another main query.

```sql
SELECT
  user_id,
  COUNT(*) AS order_count
FROM
  orders
GROUP BY
  user_id
```

The main query will now use this subquery like this:

```sql
SELECT
  order_count
FROM
  (
    SELECT
      user_id,
      COUNT(*) AS order_count
    FROM
      orders
    GROUP BY
      user_id
  ) AS p
```

Now we are provided with a single column of values, and this means that we can now use the `AVG` aggregate function on the column.

```sql
SELECT
  AVG(order_count)
FROM
  (
    SELECT
      user_id,
      COUNT(*) AS order_count
    FROM
      orders
    GROUP BY
      user_id
  ) AS p
```

> There is a way to solve this query without a subquery.

## Subqueries in `JOIN`

The `JOIN` clause can receive any subquery that returns a result compatible with the `ON` clause. Let's go over an example. This is a query to find all the users that have ordered product ID 3:

```sql
SELECT user_id
FROM orders
WHERE product_id = 3;
```

This returns a table with one column called `user_id` containing some rows. We are now going to use this query as a subquery for another query like:

```sql
SELECT first_name
FROM users
JOIN (subquery-compatible-with-on) AS o
ON o.user_id = users.id
```

So the final query would be like:

```sql
SELECT
  first_name
FROM
  users
  JOIN (
    SELECT
      user_id
    FROM
      orders
    WHERE
      product_id = 3
  ) AS o ON o.user_id = users.id;
```

> This example can be solved without using any subquery. It is just an example.

## Subqueries in `WHERE`

This probably the most useful case of using subqueries. Let's go over an example: Show the ID of orders that involve a product with a price/weight ratio greater than 5.

Using subqueries inside a `WHERE` clause will require you to understand and remember how this clause works in general. The `WHERE` clause is used to filter some rows of table. There are a couple of operators that can be used inside a `WHERE` clause.

The structure of data allowed to be returned by a subquery inside a `WHERE` statement changes depending on the comparison operator used.

Going back to the example, we are first going to list all the products that have price/weight ration greater than 5.

```sql
 SELECT id
 FROM products
 WHERE price / weight > 5;
```

This returns a single column of values. This structure of data can be used inside `IN` operator within a `WHERE` statement.

```sql
SELECT id
FROM
  orders
WHERE
  product_id IN (
    SELECT
      id
    FROM
      products
    WHERE
      price / weight > 50;
  );
```

So the `IN` operator can only receive a list of values, that is a single column of values.

> This example could be solved using a simple `JOIN` statement.

Let's give you a table so you can easily see what data structure you need to provide for each operator that you can use in a `WHERE` statement.

| Operator in the `WHERE` statement | Structure of data of subquery |
| --------------------------------- | ----------------------------- |
| >                                 | Single value                  |
| <                                 | Single value                  |
| >=                                | Single value                  |
| <=                                | Single value                  |
| =                                 | Single value                  |
| <> or !=                          | Single value                  |
| IN                                | Single column                 |
| NOT IN                            | Single column                 |
| > ALL/SOME/ANY                    | Single column                 |
| < ALL/SOME/ANY                    | Single column                 |
| >= ALL/SOME/ANY                   | Single column                 |
| <= ALL/SOME/ANY                   | Single column                 |
| = ALL/SOME/ANY                    | Single column                 |
| <> ALL/SOME/ANY                   | Single column                 |

### Examples

Show the name of all products with a price greater than the average product price.

```sql
SELECT
  name
FROM
  products
WHERE
  price > (
    SELECT
      AVG(price)
    FROM
      products
  );
```

Let's go over another example: Show the name of all products that are not in the same department as products with a price less than 100.

```sql
SELECT
  name, department
FROM
  products
WHERE
  department NOT IN (
    SELECT
      department
    FROM
      products
    WHERE
      price < 100
  );
```

Another example: Show the name, department, and price of products that are more expensive than all products in the Industrial department. This example can be solved with different approaches, but we are now trying to use an operator in the `WHERE` statement that we have not used before, and that is `> ALL`. Remember that this operator can receive a single column of values resulting from a subquery:

```sql
SELECT name, department, price
FROM
  products
WHERE
  price > ALL (
    SELECT
      price
    FROM
      products
    WHERE
      department = 'Industrial'
  );
```

Let's now go over another example where you can use the `SOME` operator to receive a subquery: Show the name of products that are more expensive than at least one product in the Industrial department.

```sql
SELECT name, department, price
FROM
  products
WHERE
  price > SOME (
    SELECT price
    FROM
      products
    WHERE
      department = 'Industrial'
  );
```

## Correlated subquery

To understand what correlated subqueries, let's try to solve another example: Show the name, department, and price of the most expensive product in each department.

```sql
SELECT
  name,
  department,
  price
FROM
  products AS p1
WHERE
  p1.price = (
    SELECT
    FROM
      products as p2
    WHERE
      p2.department = p1.department
  )
```

What makes this query special, is that we are using aliases for each of the `FROM` statements, so that we can refer to them in both the subquery and the main query in order to do some filtering. This means that the two queries are working in relation to each other, like a double-nested loop.

> In the subquery, you can refer to the alias introduced in the main query.

Let's go over another example: Without using a join or a group by, print the number of orders for each product.

```sql
SELECT
  p1.name,
  (
    SELECT
      COUNT(*)
    FROM
      orders AS o1
    WHERE
      o1.product_id = p1.id
  ) AS num_orders
FROM
  products AS p1
```

As you can see, correlated subqueries can also be used inside the `SELECT` statement.

## `SELECT` without a `FROM` or `JOIN`

As long as a subquery returns one single value, you can use it in a `SELECT` statement without using any `FROM` or `JOIN` afterwards. But why would you need this? Let's go over an example.

```sql
SELECT
  (
    SELECT
      MAX(price)
    FROM
      products
  );
```

This will give you one column with only one value which is the maximum price of all the products. Let's go over another example: Print the ratio of maximum price on minimum price.

```sql
SELECT
  (
    SELECT
      MAX(price)
    FROM
      products
  ) / (
    SELECT
      MIN(price)
    FROM
      products
  );
```

You will need to use a single `SELECT` with subqueries whenever you want calculate one row of values or calculate the result of some math around some different values combined together.

# Selecting `DISTINCT` records

The `DISTINCT` keyword is always places inside a `SELECT` clause. `DISTINCT` will return all the different unique values inside a column.

For instance, if you want to see what unique departments there are in your products table, you can use this query:

```sql
SELECT DISTINCT department
FROM products;
```

You can also use this keyword to get the number of unique values.

```sql
SELECT COUNT(DISTINCT department)
FROM products
```

> `DISTINCT` is similar to `GROUP BY`. You can use `GROUP BY` instead of `DISCTINCT`, but not the other way. The difference is that `GROUP BY` can use aggregate functions to take a look at values inside each of the groups.

The `DISTINCT` keyword can receive more than one argument. This is useful when you need to get combined unique values. For instance, if you need to find every unique combination of department and name of your products:

```sql
SELECT DISTINCT department, name
FROM products;
```

> When using `DISTINCT` with more than one argument, you can no longer use the `COUNT` function on it.

# Utility operators

## `GREATEST` function

You can use the `GREATEST` function to calculate the maximum value among some given values. For instance:

```sql
SELECT GREATEST(20, 10, 30);
```

This returns `30` as result. But how would you use this on a real table? Let's go over an example: Compute the cost to ship each item in a products table. Note that shipping is the maximum of weight\*2$ or 30$.

```sql
SELECT name, weight, GREATEST(weight * 2, 30)
FROM products;
```

## `LEAST` function

You can use the `LEAST` function to calculate the minimum value among some given values.

For instance, let's say all products are on sale, and their price are the least of price\*0.5 or 400$.

```sql
SELECT name, price, LEAST(price*0.5, 400)
FROM products;
```

## `CASE` keyword

Let's go over an example to understand this: Print each product and its price. Also print a description of the price, meaning that if price is heigher then 500 then print 'hight', if it is higher than 300 then print 'medium' and else, print 'cheap'.

The `CASE` keyword is always used together with `WHEN` and `THEN` keywords.

```sql
SELECT
  name,
  price,
  CASE
    WHEN price > 600 THEN 'hight'
    WHEN price > 300 THEN 'medium'
    ELSE 'cheap'
  END
FROM
  products
```

> Any calculation is also possible inside the `WHEN` clause. IF your conditions are written in a way that some records don't satisfy any of them, they will be described as `null`.

# Postgres complex data types

## Data types

Data types determine what type of data you can store inside each table column. For instnace, a price column would probably only allow you to insert number (`INTEGER`) type.

Here is a list of data type categories you can use in Postgres. There are many different sub-types in each category:

- Numbers
- Date/Time
- Character
- Boolean
- Geometric
- Currency
- Range
- XML
- Binary
- JSON
- Arrays
- UUID

The first four categories are probably all you would have to remember to get on with Postgres.

### Numeric types

Here is a list of number data type category:

Numbers without any decimal points:

- smallint: between -32768 and +32767
- integer: between -2147583648 and -2147583647
- bigint

Numbers without decimal points but with auto increment:

- smallserial: 1 to 32767
- serial: 1 to 2147483647
- bigserial: 1 to ...

Numbers with decimal points: Here we have 2 sub-categories again

- Extreme precision:
  - decimal: 131072 digits before decimal point, 16383 after
  - numeric: 131072 before decimal point, 16383 after
- Normal precision:
  - real: 1E-37 to 1E37 with at least 6 places precision
  - double precision: 1E-307 to 1E308 with at least 15 place precision
  - float: same as real or double precision

You can use each of these sub-type names to determine what type of value you can insert into a column cell.

#### Rules on number type

There 4 main rules that you need to understand:

1. To define the `id` column of a table, mark the column as `SERIAL`.
2. To store a number with no decimal points, mark the column as `INTEGER`.
3. To store an extremely accurate number with decimals, for example for bank balances, grams of gold, or scientific calculations, mark the column as `NUMERIC`.
4. To store a number with decimals where the number doesn't need to be scientifically accurate, for example for kilograms of trash in a landfill, liters of water in a lake, air pressure in a tire, mark the column as `DOUBLE PRECISION`.

Let's now explore some detailed rules. To do this we need to execute some queries inside PGAdmin.

If you go on and execute these queries, you will see that Postgres will automatically try to infer the exact type of number that you have inserted.

```sql
SELECT (2);
-- integer

SELECT(2.0);
-- numeric
```

You can force Postgres to treat your value in a differnt way using the `::` operator.

```sql
SELECT (2.0::INTEGER);
```

Now postgres will follow your command and assign `integer` type to a value which previously was considered `numeric`.

Now try this query:

```sql
SELECT (2.0::SMALLINT);
```

Postgres will now treat your value as a small integer and convert it accordingly to `2` with no decimal point as `smallint`.

Inserting a value that is out of range for a specific type would result in error:

```sql
SELECT (999999::SMALLINT);
-- ERROR:  smallint out of range
```

Let's now explore some rules around decimals. Take this query as an example:

```sql
SELECT (1.99999::REAL - 1.99998::REAL);
-- returns 0.00001001358 (!)
```

You would normally expect the result to be `0.00001`. But what you actually get is different: `0.00001001358`. Why? Because Postgres, treats `REAL`, `DOUBLE PRECISION`, and `FLOAT` with floating point math. This kind of calculations are notorious for being womewhat inaccurate. However, since they are very efficient and fast for running calculations, they are used when you don't need extreme precision in your numeric values. Now to calculate the query precisely:

```sql
SELECT (1.99999::DECIMAL - 1.99998::DECIMAL);
SELECT (1.99999::numeric - 1.99998::numeric);
-- both return 0.00001
```

### Character types

Here is a list of data types of character category:

- To store a fixed length of 5 (or any other length) characters, mark the column as `CHAR(5)`. If a string with longer length is provided, Postgres will trim characters until it reaches the length of 5. If a string with shorter length is provided, Postgres will insert spaces to the right side until it reches the correct length.
- To insert any length of string, mark the column as `VARCHAR`.
- To insert a fixed length of 40 (or any other length) characters, mark the column as `VARCHAR(40)`. Works similar to `CHAR(40)`
- To store any length of string, mark the column as `TEXT`.

Take this query as an example:

```sql
SELECT ('alksjdhfgkwshegdf'::CHAR(3))
-- returns 'alk'

SELECT ('a'::CHAR(3))
-- returns 'a  '

SELECT ('asdfgsdfgsdfdfgsxdfg'::VARCHAR(5))
-- returns 'asdfg'

SELECT ('asdfgsdfgsdfdfgsasdfgsdfgsdfgadfsfgasdfgadfgxdfg'::TEXT)
-- returns 'asdfgsdfgsdfdfgsasdfgsdfgsdfgadfsfgasdfgadfgxdfg'
```

> There is no performance difference between these difference character types, which is unlike other databases. Just use the type that suits the needs of your application. Adding character length limit will not change anything regarding the performance, it is just a layer of data validation before it gets inserted into the database.

### Boolean types

When we talk about booleans, we are only talking about `true`, `false` and `null`. But we can also set some special values and tell Postgres to treat them as boolean. It will basically convert them into true or false.

You can provide a string of `yes` to Postgres and tell it to treat it as true. For instance, take the query below as an example:

```sql
SELECT ('yes'::BOOLEAN)
-- returns true as boolean

SELECT ('y'::BOOLEAN)
-- returns true as boolean

SELECT ('on'::BOOLEAN)
-- returns true as boolean


SELECT (1::BOOLEAN)
-- returns true as boolean

SELECT ('no'::BOOLEAN)
-- returns false as boolean

SELECT ('n'::BOOLEAN)
-- returns false as boolean

SELECT ('off'::BOOLEAN)
-- returns false as boolean

SELECT (0::BOOLEAN)
-- returns false as boolean
```

Storing `null` in a boolean typed cell, just means that there is no data. It is still boolean, but it is neither true nor false.

### Date/time types

Postgres can store date, time, time of timezones, and timestamps. Postgres is perfectly flexible. You can provide Postgres with various formats of strings and it would be translated into a date or time format. Take the examples below:

```sql
SELECT ('NOV-20-1980'::DATE);
-- returns 1980-11-20 as date

SELECT ('NOV 20 1980'::DATE);
-- returns 1980-11-20 as date

SELECT ('NOV 20, 1980'::DATE);
-- returns 1980-11-20 as date

SELECT ('1980 NOV 20'::DATE);
-- returns 1980-11-20 as date
```

Here are some examples to store time values with no time zone:

```sql
SELECT ('01:23'::TIME);
-- returns 01:23:00 as time without time zone

SELECT ('01:23 PM'::TIME);
-- returns 13:23:00 as time without time zone

SELECT ('01:23 PM'::TIME WITHOUT TIME ZONE);
-- you can explicitely determine the type of time format

SELECT ('01:23:30 PM'::TIME);
-- returns 13:23:30
```

To use time with time zone you would have to determine the name of the time zone in the string that you provide. Postgres will then convert your time to UTC value. So take these examples:

```sql
SELECT ('01:23:30 AM EST'::TIME WITH TIME ZONE);
-- returns 01:23:30-05:00 as time with time zone

SELECT ('01:23:30 AM PST'::TIME WITH TIME ZONE);
-- returns 01:23:30-08:00 as time with time zone

SELECT ('01:23:30 AM z'::TIME WITH TIME ZONE);
-- returns 01:23:30+00:00 (z is UTC) as time with time zone

SELECT ('01:23:30 AM utc'::TIME WITH TIME ZONE);
-- returns 01:23:30+00:00  as time with time zone
```

You can also store timestamp with or without a time zone attached to it. You can provide a string containing a date in any format and a time in any format and, optionally, a time zone. See examples below:

```sql
SELECT ('NOV-20-1980 1:23 AM PST'::TIMESTAMP WITH TIME ZONE);
-- returns 1980-11-20 12:53:00+03:30 as timestamp with time zone
```

#### Calculations with date and time

Postgres has many built-in functions to perform calculations on date and time values. There is actually a data type in Postgres called `INTERVAL`. It can hold the date/time duration between two date/time values.

```sql
SELECT ('1 day'::INTERVAL);
-- returns 1 day as interval

SELECT ('1 D 20 H'::INTERVAL);
-- returns 1 day 20:00:00 as interval
```

But this is not useful by itself. It only becomes useful when you can calculate durations.

```sql
SELECT ('1 D 20 H'::INTERVAL) - ('1 D'::INTERVAL)
-- returns 20:00:00

SELECT
	('NOV-20-1980 1:23 AM EST'::TIMESTAMP WITH TIME ZONE)
	-
	('1 D'::INTERVAL)
  -- returns 1980-11-19 09:53:00+03:30 as timestamp with time zone

  SELECT
	('NOV-20-1980 1:23 AM EST'::TIMESTAMP WITH TIME ZONE)
	-
	('NOV-10-1980 1:23 AM EST'::TIMESTAMP WITH TIME ZONE)
  -- (same time zones) returns 10 days as interval

  SELECT
	('NOV-20-1980 1:23 AM EST'::TIMESTAMP WITH TIME ZONE)
	-
	('NOV-10-1980 5:43 AM PST'::TIMESTAMP WITH TIME ZONE)
  -- (different time zones) returns 9 days 16:40:00 as interval
```

# Database-side validation and constraints

When a user sends some data to the server to be stored on the database, there should be some validation process on the way in order to prevent not relevant values or types to be inserted into the database. For instance, the price of a product can never have a negative value, or some fields are strictly required and cannot be left empty. There are some solutions for this. In some situations you can do validations on the back-end (server) while in others you may do it directly inside the database. This was the first scenario. The second scenario is a bit different.

Imagine the company doesn't have the required resources to create a user interface and web server to power it. So data would have to be inserted directly into the database by an admin user. Now that data validation should be performed by the database itself. Postgres has a way of implementing data validation.

So, where should you implement validations eventually? The best approach is to spread validations accross all three. There are benefits for doing it in both:

| Web server                                        | Database                                                             |
| ------------------------------------------------- | -------------------------------------------------------------------- |
| Easier to express more complex validation         | Validation still applied even if you connect with a different client |
| Far easier to apply new validation rules          | Guaranteed that validation is always applied                         |
| Many libraries to handle validation automatically | Can only apply new validation rules if all existing rows satisfy it  |

## Row-level validation

Row-level validation is done when a record is inserted into a table. Here is a list of things we can check for when a row is being inserted or updated in a table:

- Is a given value defined?
- Is a value unique in its column?
- Is a value `>`, `<`, `>=` or `<=` sine other value?

All the examples and solutions mentioned in this section are applied in PGAdmin software.

### Applying null constraint

As an example, we create a table for products with id, name, price and weight as columns.

```sql
CREATE TABLE products (
	id SERIAL PRIMARY KEY,
	name VARCHAR(40),
	department VARCHAR(40),
	price INTEGER,
	weight INTEGER
);
```

Now as you try to insert a row into this database like below, without defining the product's price, you will see that Postgres will simply accept your insertion and put `null` for the product's price in the table.

```sql
INSERT INTO products (name, department, weight)
VALUES
	('Pants', 'CLothes', 3);
```

But we now want to apply a null constraint so as to prevent such behavior for a value that we don't want to be left undefined. There are 2 ways to apply this constraint:

1. You can update the create table statement: mark your required column as `NOT NULL`.

```sql
CREATE TABLE products (
	id SERIAL PRIMARY KEY,
	name VARCHAR(40),
	department VARCHAR(40),
	price INTEGER NOT NULL,
	weight INTEGER
);
```

2. You can update a table that was created previously, using `ALTER` and `SET` command:

```sql
ALTER TABLE products
ALTER COLUMN price
SET NOT NULL;
```

There is a gotcha around this approach. When you are using this command to update the configuration of a previously created table, you will end up in an error if the target column already has null values inserted into it. The error would say: "column "price" of relation "products" contains null values". In this situations there are two options:

1. Find all the rows that have `null` for the target column and delete them. Then, you can run the command above.
2. Fina all the rows that have `null` for the target column and change their null to something else, like `99999` or anything special that will remind you that you need to update them with real data.

As for the second approach, you can use the query below to update the null values for the price column:

```sql
UPDATE products
SET price = 9999
WHERE price IS NULL;
```

> Note that you cannot write `WHERE price = NULL` to find the null prices. To check this, you need to use the `IS` operator.

You can now run the command above to update the null constraint on the table. Now you will no longer be able to leave the price field empty as you insert a row into the table:

```sql
INSERT INTO products (name, department, weight)
VALUES
	('Shoes', 'Clothes', 5);
  -- returns an error: violates not-null constraint
```

### Default column values

In some situations, the user/admin might not really know what value they should insert for a specific column. Nevertheless, the column should not be left with a null value. So you would have to set a default value for the columns that are required, but the user/admin might not always know what to insert. To set default values for a column you have two options:

1. You can update the create table statement: mark your required column as `DEFAULT` with a value:

```sql
CREATE TABLE products (
	id SERIAL PRIMARY KEY,
	name VARCHAR(40),
	department VARCHAR(40),
	price INTEGER DEFAULT 999,
	weight INTEGER,
);
```

2. You can update a previously created table using the `ALTER` and `SET` command:

```sql
ALTER TABLE products
ALTER COLUMN price
SET DEFAULT 999;
```

You can now insert a row into the table without having to provide a value for the price column, as it would contain `999` instead of `null`.

### Appylying unique constraint

If you need the values of a specific column of a table to be unique, you can set this constraint for your table. For instance, you might need to have unique names for the product tables. Again you can do this in two situations:

1. You can update the create table statement: marke the target column as `UNIQUE`:

```sql
CREATE TABLE products (
	id SERIAL PRIMARY KEY,
	name VARCHAR(40) UNIQUE,
	department VARCHAR(40),
	price INTEGER,
	weight INTEGER,
);
```

2. You can update a previously created table using the `ALTER` and `ADD` command:

```sql
ALTER TABLE products
ADD UNIQUE (name);
```

There is a gotcha around this commnad. You cannot execute this command unless all values inside the target column are already unique. So if you have duplicate values inside the target column, you would have to clean them up beforehand, otherwise you would end up in an error.

#### Multi-column uniqueness

You may want all the products in your table to have a unique combination of names and departments.

```sql
ALTER TABLE products
ADD UNIQUE (name, department);
```

### Droping constraints

In order to drop a constraint applied to your table, you can use the `DROP CONSTRAINT` command followed by the constraint name. You can find the constraint name by looking into the _Constraints_ section of your table inside PGAdmin.

```sql
ALTER TABLE products
DROP CONSTRAINT products_name_key;
```

> To see the updated list of your table constraints in PGAdmin, you would have to refresh your table in the Object Explorer panel.

### Adding validation check

When a value is inserted or updated, you can validate if the value is `>`, `>=`, `<`, `<=` than another value. For instance, to prevent the value of a column from being negative, you can add the proper validation. Again, there are two options:

1. You can update the create table statement: mark the target column with `CHECK` along with some validation inside `( )`.

```sql
CREATE TABLE products (
	id SERIAL PRIMARY KEY,
	name VARCHAR(40) UNIQUE,
	department VARCHAR(40),
	price INTEGER CHECK (price > 0),
	weight INTEGER,
);
```

2. You can update a previously created table using the `ALTER` and `ADD` command:

```sql
ALTER TABLE products
ADD CHECK (price > 0);
```

There is a gotcha around this command. It only works when you are adding or updating a row. It does not act on already existing rows.

#### Checks over multiple columns

This is when your validation check involves the value inside more than one single column. To apply this kind of check at the time of creating a table:

```sql
CREATE TABLE orders (
	id SERIAL PRIMARY KEY,
	name VARCHAR(40) NOT NULL,
	created_at TIMESTAMP NOT NULL,
	est_delivery TIMESTAMP NOT NULL,
	CHECK (created_at < est_delivery)
);
```

With this validation check, this query will work fine:

```sql
INSERT INTO orders (name, created_at, est_delivery)
VALUES
	('Shirt', '2000-NOV-20 01:00AM', '2000-NOV-25 01:00AM')
```

But this one won't work:

```sql
INSERT INTO orders (name, created_at, est_delivery)
VALUES
	('Shirt', '2000-NOV-20 01:00AM', '2000-NOV-10 01:00AM')
  -- est_deliver is not greater than created_at time.
```

# Database structure design patterns

From now on we are going to use some more advanced featrues that require more complicated database structure and schema to work. This means that we are going to work with a lot more data and more tables. You are now going to decide how many tables you should create, how they would be related, define columns and data types for the columns.

When working with many tables in a database, it is challenging to keep the structure/name of them in your head. So it is nice to document your database structure somehow. For this, you can use a **schema designer** to help guide your design.

Here is a list of schema designer software you can use:

1. dbdiagram.io
2. drawsql.app
3. sqldbm.com
4. quickdatabasediagrams.com
5. ondras.zarovi.cz/sql/demo

In this section of this tutorial we are going to simulate the database structure of Instagram.

## Building some schema

You can use the code below to create your diagrams in dbdiagram.io web application.

```sql
Table users {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  updated_at TIMESTAMP
  username VARCHAR(30)
  bio VARCHAR(400)
  avatar VARCHAR(200)
  phone VARCHAR(25)
  email VARCHAR(40)
  password VARCHAR(50)
  status VARCHAR(15)
  -- status field might need to be changed to enum type
}

Table posts {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  updated_at TIMESTAMP
  url VARCHAR(200)
  user_id INTEGER [ref: > users.id]
}

Table comments {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  updated_at TIMESTAMP
  contents VARCHAR(240)
  user_id INTEGER [ref: > users.id]
  post_id INTEGER [ref: > post.id]

}
```

> Note that we are not storing data about a user's number of posts, number of followers and number of followings. We are not going to create a column in the users table to store a number representing these amounts. Whenever you need some data that can be calculated based on the exisiting data in a database (this is called **Derived Data**), you usually don't create a column in a table to store derived data. When you need the data, you send a query to the database to calculate it right away.
>
> In this situation we have a users table, a posts table including a `user_id` column that points to specific users, and a `followers` table with a `user_id` column pointing to specific users. You can then use the queries below to count the number of posts and followers related to a specific user, for example user with ID 123. We will implement the following system at the end of this section.

```sql
SELECT COUNT(*)
FROM posts
WHERE user_id = 123;

SELECT COUNT(*)
FROM followers
WHERE user_id = 123;
```

## Going through a real-wold example

### How to build a like system

Here are some rules around likes:

1. Each user can like a specific post a single time
2. A user should be able to unlike a post
3. Need to be able to figure out how many users like post
4. Need to be able to list which users like a post
5. Something besides a post might need to be liked (comments, maybe?)
6. We might want to think about dislikes or other kinds of reactions

#### How not to design a like system

You should not add a `likes` column to the posts table. way you would have these problems:

- No way to make sure a user likes a post only once
- No way to make sure a user can only unlike a post they have liked
- No way to figure out which users like a post
- No way to remove a like if a user gets deleted

Overall, we would have no idea who liked what and this is not good!

#### Designing a like system

It is the best practice to have a table of likes, including `id`, `user_id`, and `post_id` columns. It is extremely important to note that within this table, we must have unique combinations of `user_id` and `post_id` so that each user can only like a post once. It means that the table would have to be limited to a `UNIQUE(user_id, post_id)` constraint.

This design also works fine for a favorites or a bookmarks table. It would basically mean the same thing, although they are different features.

However, this design has its limits too:

- You cannot define different kinds of reactions, such as dislike, sad about or stuff like that.
- You cannot implement a feature where a comment could be liked. Right now only a post can be liked.

So to adjust the design above to account for the mentioned limitations, you can implement a table called reactions including `id`, `user_id`, `post_id`, and `type`. The type column can contain a limited variation of values which could be strings as `like`, `love`, `care`, `funny` and `sad`. Postgres has a special data type to make sure values inserted into this column will always be one of these, and that is **enum**.

Now to address the option for users to be able to like a post or a comment. Let's try three solutions here.

##### Polymorphic associations

Polymorphic associations are generally not recommended, but they are still in use in some applications you will work on.

With this approach you will implement a table for likes that includes `id`, `user_id`, `liked_id` and `liked_type`. The `liked_type` will either contain `post` or `comment` string values in it. But there is a huge problem with this.

Remember the concept of **data consistency** that we talked about when discussing foreign keys? When you implement a `post_id` in a likes table, you are actually defining the column as containing the foreign key relating to a specific row in the posts table. So when you insert a row in the likes table with a particular `post_id`, Postgres will first look into the posts table to see if a post with that ID actually exists. But with the `liked_id` column that we implemented in our recent design we cannot define up front to which table the foreign key is actually relating to: posts or comments? We cannot know until a row is already recorded into the table. So we cannot apply the foreign key definition to the `liked_id` column when creating the table, it would just be a simple integer column. Without foreign keys, we lose data consisitency. This is not good. We should think of another design.

> Ruby on Rails projects make use of polymorphic associations like this a descent amount!

##### Alternative design

We could have a likes table which includes these columns: `id`, `user_id`, `post_id`, `comment_id`. Now when a like is attached to a post, the post ID wil be inserted and the comment id will be left `NULL` and vice versa. Note that both `post_id` and `comment_id` columns will be foreign keys relating to the posts and comments table respectively.

We might want to add a little bit of validation when a record is being inserted into this likes table. We can make sure either a post or a comment ID is defined in the record. We can also check for situations when both post and comment ID columns are filled with foreign key data in one single record. We can also check if both are left empty. This design gives us the ability to check all these scenarios. However, writing a check for these scenarios is a bit difficult:

```sql
Add CHECK of
(
  COALESCE((post_id)::BOOLEAN::INTEGER, 0)
  +
  COALESCE((comment_id)::BOOLEAN::INTEGER, 0)
) = 1
```

The `COALESCE` function receives two arguments and returns the first non-null value between them.

So the downside to this approach is that if later we are going to allow a user to like many more things, we would have trouble adding them to the table and validate record entries.

##### Easier alternative

It is a lot easier to create a couple of tables. A tables called `posts_likes` and another one called `comments_likes`. The first would include `id`, `user_id` and `post_id`, and the second one would include `id`, `user_id` and `comment_id`.

This is the most straight forward solution. This way we can add more tables if we would like to make users able to like other things than posts and comments. Data consistency is fully considered also.

The downside to this approach is that if you ever want to write a query where you will aggregate all the different kinds of likes inside the application, you would have to use a **union** or a **view** (We will talk about views later).

##### Conclusion

The second approach might be the optimal solution to this problem. We don't really need to create different tables for likes on posts and likes on comments, because there is no additional information in one compared to the other. Liking a post and a comment are pretty similar to each other; They are actually the same.

Let's now implement the design:

```sql
Table users {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  updated_at TIMESTAMP
  username VARCHAR(30)
}

Table posts {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  updated_at TIMESTAMP
  url VARCHAR(200)
  user_id INTEGER [ref: > users.id]
}

Table comments {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  updated_at TIMESTAMP
  contents VARCHAR(240)
  user_id INTEGER [ref: > users.id]
  post_id INTEGER [ref: > post.id]
}

<!-- THE FINAL APPROACH TO LIKES -->
Table likes {
  id SERIAL  [pk, increment]
  created_at TIMESTAMP
  user_id INTEGER [ref: > users.id]
  comment_id INTEGER [ref: > comments.id]
  post_id INTEGER [ref: > posts.id]
}
```

### How to build a mention system

As an intermediary step, let's make the posts able to receive captions and location coordinates. So we would simply have to add some columns to the posts table.

```sql
Table posts {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  updated_at TIMESTAMP
  url VARCHAR(200)
  user_id INTEGER [ref: > users.id]
  caption VARCHAR(240)
  lat REAL
  lng REAL
}
```

> The `REAL` type is numeric type with decimals. With this type, you are not guaranteed to have an absolutely precise number, but you will have precision at the level of 6 decimal digits. over that, Postgres will start to round off the value. For string location coordinates, this is probably enough as for data accuracy.

Now about the mention or tag system, we are going to consider two types of tags:

1. Tag on a photo: tagging a user on a photo requires us to store 3 piece of data related to this kind of tag. First, the username that was tagged, and then the point on the photo to which the tag is attached. So we would need the `user_id`, `x` and `y` in a tags table.
2. Tag in a post caption: Tags in post caption usually initiate with a `@` symbol and is highlighted. But this does not mean that we necessarily need to store something in our database. Highlighting a mention in a post caption is something that can be done easily on the front-end. But if any of the situations below are going to happen, you might need to store some data about this kind of tag in your database:

- Need to show a list of posts a user was mentioned in?
- Need to show a list of the most-often mentioned users?
- Need to notify a user when they have been mentioned?

It might seem that the two types of tags are very different. But thinking twice about it, they are actually very similar! In both situations, we are actually tag a user to a post. Their behavior is pretty similar.

#### One table for both

In this approach, we would create a table that includes both photo tags and caption tags. This table will have `id`, `user_id`, `post_id`, `x`, `y` columns, where for photo tags, columns `x` and `y` will contain numeric values, while in captions tags they are left `NULL`.

#### Separate tables

In this approach we would have a `photo_tags` table and a `caption_tags` table.

#### Conclusion

To decide between the two solutions, let's consider these:

- Do you expect to query for caption tags and photo tags at different rates? If that is the case, you might need to split the two tags in separate tables.
- Will the meaning of a photo tag change at some point? If a tag is likely to change its functionality over time, you might need to store it in a separate table. For instance, if the tag will receive likes in future.

So let's go with the second solution:

```sql
Table photo_tags {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  updated_at TIMESTAMP
  post_id INTEGER [ref: > posts.id]
  user_id INTEGER [ref: > users.id]
  x INTEGER
  y INTEGER
}

Table caption_tags {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  user_id INTEGER [ref: > users.id]
}
```

### How to build a hashtag system

Hashtags can be inserted into photo captions, comments, and profile biographies. One solution might be to create a table for hashtags in each, so `hashtags_posts`, `hashtags_comments`, `hashtags_users` tables. The post hashtags table would contain `id`, `title` and `post_id` columns and so on.

We could also implement a polymorphic association pattern and combine the three tables in one. Again to decide between these two solutions, you might ask yourself: Do you expect to run a query to see what posts/comments/users contain a given hashtag? If yes, then the first solution is fine.

We know that hastags are used only in the search feature. There you can search hastags and what you get back is a list of posts that contain that hashtag. So a hashtag is only useful for seaching posts with that hashtag, and not comments or user profiles that have used the hashtag.

So we need the `hashtag_posts` table, but we don't need the two tables `hashtags_comments` and `hashtags_users` separately. We can just get rid of them in our design.

To model the relationship between a hashtag and a post, we can do it in 2 ways:

1. A hashtags table containing `id`, `title`, `post_id` columns. This works fine, but for performance reasons, there might be a better way. In this first solution, we are very likely to end up with a table containing a lot of duplicates values in the `title` column. This will cause us to use up more storage space.
2. A table for hashtags including `id` and `title`. Another table containing posts, and another table connecting the two resources called `hashtags_posts` including `id`, `hashtag_id` and `post_id`. This third table is called a **join table**. In this second solution, we will also have duplicate IDs in the `hashtag_id` column but it will use a lot less storage space as an integer takes up much less space in storage.

Let's now implement the design:

```sql
Table hashtags {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  title VARCHAR(20)
}

Table hashtags_posts {
  id SERIAL [pk, increment]
  hashtag_id INTEGER [ref: > hashtags.id]
  post_id INTEGER [ref: > posts.id]
}
```

### How to build a follower system

There are generally two main rules around a follower system.

1. Each user can follow another user only once.
2. No user can follow themselves.

To implement a follower system, we create a table containing `id`, `user_id` and `follower_id` columns. `user_id` points to the user who is being followed. In this table, we have to implement this check:

```sql
CHECK (user_id != follower_id)
```

You can also check if a user follows another user only once:

```sql
UNIQUE(user_id, follower_id)
```

Remember to name the columns properly so that it would be understood easily. `user_id` might better be replaced by `followed_id` or anything else.

Let's now implement the followers table design:

```sql
Table followers {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  followed_id INTEGER [ref: > users.id]
  follower_id INTEGER [ref: > users.id]
}
```

# Implementing Design in Postgres

## Creating tables with checks

### Creating the users table

We are now going to create the users table. You can use the query below:

```sql
 CREATE TABLE users (
	id SERIAL PRIMARY KEY,
	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
	updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
	username VARCHAR(30) NOT NULL,
	bio VARCHAR(400),
	avatar VARCHAR(200),
	phone VARCHAR(25),
	email VARCHAR(40),
	password VARCHAR(50),
	status VARCHAR(15),
	CHECK(COALESCE(phone, email) IS NOT NULL)
)
```

Note that `CURRENT_TIMESTAMP` is a Postgres reserved variable name which holds the value of the current timestamp and we have provided it as a default value of `created_at` column if the value is not provided by the client who is creating the user row.

Also note that the `CHECK` at the end is making sure that at least one of the fields (phone or email) is provided with a value and not both of them are left null.

> Reminder: `NOT NULL` means that a value must be provided (empty strings are values).
>
> Reminder: `DEFAULT` means provide a default value if an `INSERT` statement does not give one.
>
> Note: If it does not matter if a value exists, you don't need to mark `NOT NULL` or provide `DEFAULT`.
>
> Note: When you always want a value but it should be optional, you can apply both `NOT NULL` and `DEFAULT`.

### Creating the posts table

You can use the query below to create the posts table:

```sql
CREATE TABLE posts (
	id SERIAL PRIMARY KEY,
	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
	updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
	url VARCHAR(200) NOT NULL,
	caption VARCHAR(240),
	lat REAL CHECK(lat IS NULL OR (lat >= -90 AND lat <= 90)),
	lng REAL CHECK(lat IS NULL OR (lat >= -180 AND lat <= 180)),
	user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE
);
```

> Note that valid values of geographic latitude can go between -90 and 90. Valid values for geographic longitude go between -180 and 180.

> Reminder: If rules around validating a value might change frequently, you don't need to add database-level validation. Also, if rules around validating the values are complex, you had better implement them on the server application. However, if you want to make sure we have the right type or domain of values, you can use database-level validation. This is what we are doing for the `lat` and `lng` columns.
>
> Reminder: `ON DELETE CASCADE` makes sure if a user is deleted from the database, all posts related to that user will be deleted also.

### Creating the comments table

Here is the query to create the comments table:

```sql
CREATE TABLE comments (
	id SERIAL PRIMARY KEY,
	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
	updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
	contents VARCHAR(240) NOT NULL,
	user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
	post_id INTEGER NOT NULL REFERENCES posts(id) ON DELETE CASCADE
)
```

### Creating the likes table

Here is the query to create the likes table:

```sql
CREATE TABLE likes (
	id SERIAL PRIMARY KEY,
	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
	user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
	post_id INTEGER REFERENCES posts(id) ON DELETE CASCADE,
	comment_id INTEGER REFERENCES comments(id) ON DELETE CASCADE,
	CHECK(
		COALESCE((post_id)::BOOLEAN::INTEGER, 0)
		+
		COALESCE((comment_id)::BOOLEAN::INTEGER, 0)
		= 1
	),
	UNIQUE(user_id, post_id, comment_id)
)
```

> Question: what is this check doing differently than the check I implemented in the users table?

### Creating the photo tags and caption tags tables

To create the photo tags table you can use this query:

```sql
CREATE TABLE photo_tags (
	id SERIAL PRIMARY KEY,
	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
	updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
	user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
	post_id INTEGER NOT NULL REFERENCES posts(id) ON DELETE CASCADE,
	x INTEGER NOT NULL,
	y INTEGER NOT NULL,
	UNIQUE (user_id, post_id)
)
```

> Reminder: the `UNIQUE` check is making sure that a user gets tagged only once in a single post.

To create the caption tags table you can use this query:

```sql
CREATE TABLE caption_tags (
	id SERIAL PRIMARY KEY,
	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
	user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
	post_id INTEGER NOT NULL REFERENCES posts(id) ON DELETE CASCADE,
	UNIQUE (user_id, post_id)
)
```

### Creaing the hashtags table

To create a table for hashtags you can do this:

```sql
CREATE TABLE hashtags (
	id SERIAL PRIMARY KEY,
	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
	title VARCHAR(20) NOT NULL UNIQUE
)
```

To create a table for post hashtags you can do this:

```sql
CREATE TABLE hashtags_posts (
	id SERIAL PRIMARY KEY,
	hashtag_id INTEGER REFERENCES hashtags(id) ON DELETE CASCADE,
	post_id INTEGER REFERENCES posts(id) ON DELETE CASCADE,
  UNIQUE(hashtag_id, post_id)
)
```

> The `UNIQUE` check is making sure a hashtag will be recorded only once on a single post.

To create the table for followers you can do this:

```sql
CREATE TABLE followers (
	id SERIAL PRIMARY KEY,
	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
	leader_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
	follower_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
	UNIQUE(leader_id, follower_id)
)
```

> The `UNIQUE` check is making sure a user can follow another user only once.

# Managing the database

## Inserting data from a backup file

Let's first insert a lot of fake data into our implemented database design.

A backup file compatible with Postgres and PGAdmin usually has the `.sql` file extension. In order to insert this file into our implemented tables, you need to right-click on the database, click on `restore`, then locate and select the file, then turn on the options below:

1. Data options: type of objects: only data (on)
2. Data options: do not save: owner (on)
3. Query options: single transaction (on)
4. Options: disable: triggers (on)
5. Options: miscellaneous / behavior: verbose messages (on)

## Restoring a database if accidentally deleted

# Understanding the internals of Postgres

> **This and some subsequent sections of this note file have overlooked some important aspects of Postgres internals and performance subjects as they were too complicated and deep for the first learning scan. You need to fill these sections with that necessary information when you have completed your basic understanding of Postgres.**

In order to get a good performance out of your Postgres database, you have to understand what goes on inside it:

1. You have to understand how information is being stored on hard disk, and how it gets accessed.
2. What tools we have and how they work. Understanding indexes and how they work.
3. How queries are interpretted by the database and how they actually get executed.

## Where does Postgres stores data on hard disk

You can use the `SHOW` clause anytime you want to pull one configuration option out of your database.

```sql
SHOW data_directory;
-- returns C:/Program Files/PostgreSQL/17/data
```

In this directory, all the data that you have in your database is located inside the `base` folder. There are multiple folders with numeric names assigned to them, each containing data related to a separate database. If you want to understand what these numbers are you can run this query:

```sql
SELECT oid, datname
FROM pg_database;
-- returns the same set of directories you found in your file explorer.
```

Among them, you see `template0` and `tewmplate1` but we will discuss these later. You can also see the `instagram` database that you created previously. Inside this directory, you will find a lot of files that contain all the raw data of your database. But what actually are these files and what is stored in each? You can understand that by running another query:

```sql
SELECT * FROM pg_class;
```

Each of the rows you get from this query contains information about one of those files. Each file represents one individual object inside your database. The objects that you have in your database are not just tables. We also have objects for indexes, sequences, primary keys, and a couple of other things as well. You can find, for example, the file related to the users table by looking into the `relname` column and find its `oid`. Then you can go back to the file explorer and find the file there. Let's not understand how information is actually stored in this data. But before that, let's understand a little piece of terminology.

### Heaps, blocks and tuples

#### Heap or heap file

This is a file that contains all the data (rows) of a table. A heap data structure is very different than a heap file.

#### Tuple or item

This is an individual row from a table.

#### Block or page

The heap file is divided into many different blocks or pages. Each page/block stores some number of tuples, items or rows. By default, each block/page is 8kb in size regardless of how many items are stored inside it.

But why do we have this structure in our database?

## Block data layout

Let's now go deeper and understand what goes on in a block. We are going to understand how a block is physically stored on the hard drive. So let's take the file `22445` as an example. It is a heap file and there are a couple of blocks in it. We are now concentrating on one block.

When we talk about block 1, we are actually thinking about some information that is being stored on the hard disk of your computer. You hard disk stores information as binary (0s and 1s). When we talk about the structure of a block, we are essentially talking about what different 0s and 1s inside this block are actually being used to store what piece of information. It is a continuous series of 0s and 1s.

A collection of 0s and 1s at the very beginning of the block represent information about the block itself. The next set of 0s and 1s contain information about the actual data that is stored in this block, so that is information about the actual rows, but it does not contain information about the rows themselves (so no usernames, no IDs). It simply says where you can find the different rows that are stored in this block.

Then there is huge area of empty space in the block. This is a series of 0s and 1s that are not being used by the block right now. This space can eventually be used to assign some kind of user data or whatever we are trying to store inside the table.

Finally, at the very end of the block, is the actual data itself. Again it is a series of 0s and 1s where information about tuples are stored.

Let's now take a look a an actual block stored on your hard drive.

## Heap file layout

Let's map out how Postgres stores data at the binary level. You can also check a documentation at:

```
postgresql.org/docs/current/storage-page-layout.html
```

Remember that pages are 8kb in size, and a single heap file has many pages inside it.

> Some crazy stuff is written in this document and displayed in video tutorial episode. Take a look at it whenever you need.

# Indexes for performance

Now that you have seen how Postgres stores records inside individual blocks inside a heap file, you may ask, who cares? We are now going to walk through a query and understand how it is executed by Postgres and start to understand some performance characteristics of it.

Imagine we have a heap file where there are two blocks. Block 0 holds information on usernames Nancy and Alf, and block 1 holds information on usernames Jia and Riann. Now what would happen if you run this query:

```sql
SELECT *
FROM users
WHERE username = 'Riann';
```

![sql-indexes-1](/images/sql/sql-indexes-1.jpg)

The important thing to keep in mind here is that when data is inside a heap file on your hard drive, Postgres cannot examine that file in place. In other words, in order to take a look at the different users, we have to first load these users up into memory (RAM). So step 1 for a query like this would be to take a look at all of our different blocks inside the heap file. Postgres would load up all the different users into memory, and then once they are inside memory, we can start some further querying or filtering on this data.

The problem is that anytime that we load up information from our hard drive over to memory, it has a relatively large performance cost. So whenever possible, as database engineers, we try to minimize the amount of data that is being moved between the hard drive and the memory. So it is important to understand how we can limit how much data is being taken out of a heap file and placed into memory. Finally, when the data is brought into memory, we then have to walk through each individual record until eventually we find some number of rows that satisfy the criterion of having a username of `Riann`.

Anytime that we load up information from a heap file on hard drive to memory and then iterate over the records one by one, we refer to this as a **full table scan**. This frequently has poor performance, but not always. There are some situations where a full table scan is actually desirable over any alternate method. Regardless, any time that we see Postgres is doing a full table scan, we want to do a little bit of investigation and figure out whether or not there is some way that we can search for the data in some different manner, which could be more efficient to execute the query.

## What is an index

There is some way that we could load up some particular records out of a heap file without having to first load them all into memory. How would we do that? How would we somehow figure out that we want specifically user Riann without having to load up all the different records from the heap file? Maybe if we had some kind of outside tool of sorts, something that existed separate from the entire heap file or the table that could somehow tell us exactly where every user was located inside heap file.

If we had some tool like this, then we could get Postgres to go into that heap file, find just the targetted block, maybe load up the entire block and then get just user Riann out of that and we have our data!

![sql-indexes-2](/images/sql/sql-indexes-2.jpg)

Well fortunately we have this tool. It is implemented in Postgres as something called an **index**. An index is a data structure that very efficiently tells us exactly what block and index a particular record is stored at.

To understand how an index works internally, it is easiest if we approach this from how an index is initially created. It would also help you understand pros and cons of index.

## How an index is created

1. For the first step, in creating an index, you are first going to decide exactly which column we want to have a very fast lookup on. When you create an index, you create it on a very specific column that allows us to do a fast lookup on your table whenever you are doing some kind of filtering logic on the same column.

As for the query we discussed earlier:

```sql
SELECT *
FROM users
WHERE username = 'Riann';
```

you would want an index on the username column in particular because this would allow you to run queries that involve username very quickly.

Remember that you could technically have an index that takes into account the value in multiple different columns for each row, not just one. But for now we are working with index for only one column.

2. In the second step, you are going to take a look at your user table or really your heap file, and you are going to look for every single row, and you are going to extract just that one property that you want to create the index for. Then when you extract that property, you are also going to record the block and the index that we found the property at. In other words, you are going to take a look at block 0, start with Nancy, username is Nancy. So you will extract Nancy and you will record that you found that username at block 0 index 1. Then you will extract Alf and record its index, and so on to the next block and for all the data in the heap file.

Notice that the `username` values with block and index numbers are the actual information that is going to go into the index.

![sql-indexes-3](/images/sql/sql-indexes-3.jpg)

3. For the next step, you are now going to take a look at all the different values that you have extracted and sort them in a meaningful way. For strings or text, you would sort them in alphabetical order. If they were numbers, you would sort them ascendingly or descendingly.

4. For the next step, you will take the index list of records and organize them into a tree data structure. When you insert these records into the three, you are going to distribute all the records evenly among the different leaf nodes that you have inside the tree.

![sql-indexes-4](/images/sql/sql-indexes-4.jpg)

5. Last step is to add some helpers to the root node. As you see in the image above, there are two empty boxes under the root node. In them, you are going to put some directions to say whether or not someone who is trying to execute a query or find some particular record should go down to this leaf node. In other words, in each of the empty boxes, you are going to list what set of values exist inside the related leaf node. For example, in a simple explanation, you would say go to this leaf node if `'Alf' <= username < 'Nancy'`. For the other empty box you would say go to this leaf node if `'Nancy' <= username`.

![sql-indexes-5](/images/sql/sql-indexes-5.jpg)

Now when you look for the username `Riann`, this index would help you avoid loading up all the heap file into memory. Instead, you will do just a simple evaluation according to the sorted data, and that will lead you toward the correct leaf node of the index, and that in turn will lead you toward the targetted block in the heap file, without having to go over all of the blocks inside the heap file.

## Creating an actual index in Postgres

To create an index for the users table you can use this syntax:

```sql
CREATE INDEX ON users (username);
```

Using this command, the index will be, bu default, named conventionally as `users_urename_idx`. If you want to name the index manually you can use this syntax:

```sql
CREATE INDEX users_username_idx ON users (username);
```

### Droping an index

To drop a previously created index you can use this query:

```sql
DROP INDEX users_username_idx;
```

## Benchmarking queries

Benchmarking a query means to measure how long a query takes to be executed. Whenever you run a query in PGAdmin 4, you see a report at the messages section that, for example, says 'Query returned successfully in 69 msec.' However, this number is a bit misleading because it includes travel time for the query from the PGAdmin interface over to the database and back. So it includes some network travel time that does not really reflect nicely nor is really relevant on the actual execution speed of the query itself. So it is not a great metric.

In order to correctly benchmark a query, you use `EXPLAIN ANALYZE` right at the beginning of your query:

```sql
EXPLAIN ANALYZE SELECT *
FROM users
WHERE username = 'Emil30';
```

Using this syntax, what you get back is no longer records that satisfy your query. It would be just a report about the query plan and performance. At the final row of the table returned by this query, you see that the execution time is about 0.081 ms. Remember that this query is executed with the username index in place. You can repeatedly execute the query and receive some different but close execution times. Execution time 0.08 ms is really really fast.

We are now going to remove the index and repeat the query. It will now execute in 0.583ms. Note that this time is still very fast, but compared to the previous execution time, it is nearly 7 times slower. That is a lot! So using an index, made our query run 7 times faster. That is a huge improvement.

## Downsides of indexes

Although indexes help a query run a lot faster, this does not mean that you should go ahead and add an index for all columns of all your tables. Why?

Remember that creating an index means to create a tree-like structure behind the scenes. In that tree, for every row of our actual table, we extracted a piece of information along with a pointer over to some location inside our heap file. In other words, for every single row we are now storing an additional piece of information and a pointer which obviously comes with some amount of storage cost. You are using some amount of your hard disk space just to store this index.

You can actually see how much space your table and its index are occupying on your hard drive using these queries:

```sql
SELECT pg_size_pretty(pg_relation_size('users'));
-- returns 872 kb

SELECT pg_size_pretty(pg_relation_size('users_username_idx'));
-- returns 184 kb
```

So an index for the username column of the users table takes up 184 kb of space on your hard drive. Now 872kb and 184kb are not big numbers, but a table on a database for a huge application could take up to 80GB of space, and an index created on this table could take up around 18GB of space, and that is too much!

So try to use indexes whenever it makes sense regarding the performance and financial aspects of the project.

There are also other downsides associated with creating an index. Having an index can slow down the insert, update and delete operations on a specific table. Every single time that you make a change to the table, Postgres would have to update the index too. So try not to create an index for a table that is going to get updated very frequently.

Another downside is that in some scenarios Postgres is not going to use an index to speed up a query. Just because an index exists, does not guarantee that Postgres is going to actually use it. Some queries run faster without using an index at all. Let's go over an exmaple for this.

So here is a list of downsides to indexes:

1. They take a significant amount of space especially in huge databases.
2. They slow down insert, update and delete operations.
3. Postgres might not even use your index.

## Types of index

Whenever you create an index, you are actually creating a particular type of index. There are several different types of indexes in Postgres. The most common type of index is a **B-tree**. The image below shows the B-tree structure:

![sql-indexes-5](/images/sql/sql-indexes-5.jpg)

In a vast majority of times, B-tree is the type of index that you would want to create. Here is a list of every possible type of indexes, but you most likely never have to worry about other types:

| Index type | Description                                                        |
| ---------- | ------------------------------------------------------------------ |
| B-tree     | General purpost index. 99% of the time you want this               |
| Hash       | Speeds up simple equality checks                                   |
| GiST       | Geometry, full-text search                                         |
| SP-GiST    | Clustered data, such as dates - many rows might have the same year |
| GIN        | For columns that contain arrays or JSON data                       |
| BRIN       | Specialized for really large datasets                              |

## Automatically generated indexes

In two situations, Postgres automatically created indexes for your tables:

1. Postgres automatically creates an index for the primary key column of every table.
2. Postgres automatically creates an index for any `UNIQUE` constrained column of a table.

Remember that these indexes are not listed in the `indexes` section in PGAdmin.

For instance, if you have a table named `hashtags`, the index that is created for the `id` column (if it is assigned to be a primary key) would be called `hashtags_pkey`. Also if you contstraint the `title` column of the table to be `UNIQUE`, its index will be called `hashtags_title_key`. So try not to create index for these scenarios, since they get created automatically.

There is a query that you can use to see what indexes actually exist inside your database:

```sql
SELECT relname, relkind
from pg_class
WHERE relkind = 'i';
```

The `pg_class` table lists all the different objects that exist inside your database, so all the different tables, indexes, sequences and so on. `relkind` of `i` means an index object.

## Behind the scenes of indexes

When you create an index, an actual file is created on your hard drive. The file is assigned some kind of random number identifier, and it is called something like `users_username_idx`. The structure of this file is essentially the same as heap files we discussed earlier:

![sql-indexes-6](/images/sql/sql-indexes-6.jpg)

So inside this file, there 8kb pages. Each page in an index has a specific purpose, whereas in a heap file all pages are essentially the same.

The very first page in an index file is called a Meta page. This page has some information about the overall index. In addition, we have some Leaf pages and one Root page. All of these pages represent a node of the index's B-tree. This is the structure of the B-tree regarding these pages:

![sql-indexes-7](/images/sql/sql-indexes-7.jpg)

In the root page, there are some directions to direct us to a leaf page with some particular records. The leaf pages are where the actual information are stored for the index. A leaf page might have some listing of usernames and for each username there might be a pointer to where we could find the record related to that username inside the users heap file.

The pages in an index file are identical in nature to those pages in the heap file. So in each of these pages, there is a header, there is that item id index array, then some free space, then the actual items stored in each of these leaf pages.

# Query tuning

We still dont' really know when to use an index and how to really evaluate whether a given columns should get an index at all. So we are now going to learn how to evaluate what makes a good query and what makes a bad query and how we can fix a query or make it better at least a little bit by creating an index.

The first thing to do is to better understand how Postgres handles queries that we hand off to it.

## The query processing pipeline

![sql-indexes-8](/images/sql/sql-indexes-8.jpg)

In this diagram you can see the query processing pipeline. So when you start with a query like this:

```sql
SELECT *
FROM users
WHERE username = 'Alyson14';
```

This query will go through a series of steps. First, the parser, then the rewriter, then the planner, and finally an executor.

### Parser

Parser will immediately try to take all the different characters of the query string and tear it apart one by one and figure out what the meaning of every character in every word inside there. Therefore, the parser is going to make sure that what you wrote inside the query is actually a valid SQL. After this evaluating, it is going to build up something called a query tree.

A query tree is a programmatic description of the query that you are trying to run. Once the query tree is written, it is then handed off to the rewriter.

### Rewriter

The rewriter is going to take a look at the query tree and possibly make little modifications to it if Postgres thinks that certain parts could be executed a little bit more efficiently. However, what happens much more frequently is it applies the idea of views to the query tree itself. We will learn about views later...

### Planner

This is what we really care about. The goal of the planner is to take a look at the query tree, figure our what information you are trying to fetch and then come up with a series of different plans or strategies that could be used to actually get that information. In this case, the planner might realize that you are trying to get some information out of the users table based upon a username. It might say we could probably get that information very efficiently by using the user's username index and then use the references or pointers inside there to go and fetch some appropriate data from the users heap file.

Tha planner might also come up with a plan where it says rather than using an index, we could just go into the user table directly, fetch all the different users and do a one by one search.

After coming up with all these different plans, the planner is going to evaluate which one it thinks is going to execute the fastest and then choose that plan to actually run. After the planner decides on what the most efficient strategy is, it is handed off to the executor.

### Executor

This step just executes the query and get some number of rows back for you and the process finishes.

## `EXPLAIN` and `EXPLAIN ANALYZE`

We are going to use the output of the planner section to understand what makes a fast query and what makes a slow query. `EXPLAIN` and `EXPLAIN ANALYZE` are some of our best tools for understanding how a query is actually being executed and figuring out how to improve the performance of that query as well.

- Explain: Postgres will build a query plan and display information about it
- Explain analyze: Postgres will build a query plan, run it, and then display information about it including some statistics about the query performance in each row.

The two statements are for benchmarking and evaluating queries, not for use in real data fetching.

Take this query as an example:

```sql
EXPLAIN SELECT username, contents
FROM users
JOIN comments ON comments.user_id = users.id
WHERE username = 'Alyson14';
```

You well get back a table called **Query plan**. It displays the plan to execute to actually get some information. Opting for `ANALYZE`:

```sql
EXPLAIN ANALYZE SELECT username, contents
FROM users
JOIN comments ON comments.user_id = users.id
WHERE username = 'Alyson14';
```

You will get back the same table but with some additional rows about the query performance statistics like this:

```
"Hash Join  (cost=8.31..1795.11 rows=11 width=81) (actual time=0.126..11.238 rows=7 loops=1)"
"  Hash Cond: (comments.user_id = users.id)"
"  ->  Seq Scan on comments  (cost=0.00..1628.10 rows=60410 width=72) (actual time=0.018..4.027 rows=60410 loops=1)"
"  ->  Hash  (cost=8.30..8.30 rows=1 width=17) (actual time=0.028..0.029 rows=1 loops=1)"
"        Buckets: 1024  Batches: 1  Memory Usage: 9kB"
"        ->  Index Scan using users_username_idx on users  (cost=0.28..8.30 rows=1 width=17) (actual time=0.024..0.025 rows=1 loops=1)"
"              Index Cond: ((username)::text = 'Alyson14'::text)"
"Planning Time: 0.267 ms"
"Execution Time: 11.270 ms"
```

Note that at the beginning of some of the rows in the report above, there are arrows. We refer to these rows as **query nodes**. These are essentially some step where we are trying to access some data that is stored inside the database or we are trying to do some processing. In addition, the first top row (Hash Join) does not have an arrow but it is also a query node.

The correct way to read along all these rows, is to first go for the inner most row that has an arrow, which in this case is:

```
"        ->  Index Scan using users_username_idx on users  (cost=0.28..8.30 rows=1 width=17) (actual time=0.024..0.025 rows=1 loops=1)"
```

Every single place where we see on of these arrows, we imagine it is trying to access some data inside our database or inside an index and then passing that data up next to the nearest parent that has an arrow on it.

![sql-indexes-9](/images/sql/sql-indexes-9.jpg)

The hash step is then doing some processing on the data and then again sending it up to the nearest parent node which is the Hash join. At the same time with the Hash step, we also got a step which does sequential scan on comments. A sequential scan means we are going to access all the different rows inside a table. So the data acquired by this step will also be sent up to the Hash join step which is again the nearest parent node. So the Hash join step at the end is combining the results of Hash step and Sequential scan step.

Finally, the result of the Hash join step is the result of the query that we inserted into Postgres.

In addition to these steps which are marked with arrows within the Explain analyze report, there are some numbers. Let's go through these numbers for the Hash join step:

```
"Hash Join  (cost=8.31..1795.11 rows=11 width=81) (actual time=0.126..11.238 rows=7 loops=1)"
```

Hash join tells us that we are doing a hash join operation which is going to implement some kind of join process, like the same kind of join that we put into out query. Let's now go through the numbers:

1. Cost: Amount of processing power required for this step.
2. Rows: A guess at how many rows this step will produce
3. Width: A guess at the average number of bytes of each row

Now there is a mystery here. If you use `EXPLAIN` instead of `EXPLAIN ANALYZE`, you will still receive the guessed numbers in the returned result. But how do you think Postgres is able to guess the number of rows and number of bytes without executing the query? The answer is that Postgres actually keeps some very detailed statistics about what is going on inside each of your different tables. We can see how Postgres is tracking the statistics of our tables using this query:

```sql
SELECT *
FROM pg_stats
WHERE tablename = 'users';
```

`pg_stats` is a table that is created and maintained by Postgres. This table helps Postgres make close guesses about our data and query execution beforehand.

> You can either use `EXPLAIN ANALYZE` within your query string or you can use the Explain analyze feature built into the PGAdmin software which will provide you with some digrams and graphical information about the query performance.

### Understanding cost analysis

Cost refers to the amount of time to execute some part of a query plan. We will however, update this definition gradually as we try to understand this concept.

In the planner step of the query execution pipeline, a query plan is generated which is the way to execute the query. The planner might decide to use the index for the username column of the users table and then use the pointers from the index to open the user heap file and load the appropriate pages and fetch the related users. Alternatively, the planner might decide not to use the index and fetch all the users and search one by one through them. But how does Postgres decide which plan is fastest without running them?

Here is a list of actions that should be done in each scenario:

![sql-indexes-10](/images/sql/sql-indexes-10.jpg)

How can Postgres decide which scenario will be quicker? Maybe the best way to think about the number of pages that have to be loaded from the hard disk:

![sql-indexes-11](/images/sql/sql-indexes-11.jpg)

Now if there are 100 pages, then at first glance, you know that looking at the index to find the related user is probably faster and more efficient. But there is a twist here: **Loading data from random spots off a hard drive usually takes more time than loading data sequentially.** Jumping randomly all over the place on a hard drive takes more time. This makes it so that deciding on choosing one of the scenarios above will be based on a more complicated but more accurate calculation rather than just counting the number of pages in each scenario.

![sql-indexes-12](/images/sql/sql-indexes-12.jpg)

Let's now see the full equation that Postgres uses to calculate the cost of any arbitrary step of your query:

```
cost =
  (number of pages read sequentially) * seq_page_cost +
  (number of pages read at random) * random_page_cost +
  (number of rows scanned) * cpu_tuple_cost +
  (number of index entries scanned) * cpu_index_tuple_cost +
  (number of times function/operator evaluated) * cpu_operator_cost
```

You can take a look at the values of cost factors used in the equation above in the link below:

```
postgresql.org/docs/current/runtime-config-query.html
```

You can also take a look at this diagram to understand it better:

![sql-indexes-13](/images/sql/sql-indexes-13.jpg)

> These constant values for the cost factors are almost never meant to be manipulated unless you really need to.

# Common Table Expressions (CTE)

We are now moving back from Postgres internals and low-level information and we are going to discuss common table expressions which is a technique you can use to make your queries a little bit more readable.

There are 2 different forms of CTEs:

1. Simple form: used to make a query easier to understand.
2. Recursive form: used to write queries that are otherwise impossible to write.

## Simple table expressions

Let's start with an example: Show the username of users who were tagged in a caption or photo before January 7th, 2010. Also show the date they were tagged. So we are going to need the `users`, `caption_tags` and `photo_tags` tables of our database.

```sql
SELECT username, tags.created_at
FROM users
JOIN (
	SELECT user_id, created_at FROM caption_tags
	UNION ALL
	SELECt user_id, created_at FROM photo_tags
) AS tags ON tags.user_id = users.id
WHERE tags.created_at < '2010-01-07';
```

This query works fine, but look at the subquery. It is pretty long and is making the whole query hard to read. We can use a CTE to write the subquery more clearly.

```sql
WITH tags AS (
	SELECT user_id, created_at FROM caption_tags
	UNION ALL
	SELECt user_id, created_at FROM photo_tags
)
SELECT username, tags.created_at
FROM users
JOIN tags ON tags.user_id = users.id
WHERE tags.created_at < '2010-01-07';
```

So common table expressions are define using a `WITH` keyword before the main query. It produces a table that we can refer to anywhere else in our query.

> Using CTEs don't make any changes in the way the query is executed, thus no difference in performance.

## Recursive common table expressions

Recursive CTE is a bit challenging. Let's review a few things to remember first:

1. It is very different from simple CTEs.
2. It is useful anytime you have a tree or graph-type data structure.
3. You must use a `UNION` keyword when you are using recursive CTEs. Simple CTEs don't have to use a union.
4. This is a super advanced feature of SQL.

Let's go for an example here:

```sql
WITH RECURSIVE countdown(val) AS (
	SELECT 3 AS val -- initial (non-recursive) query
	UNION
	SELECT val - 1 FROM countdown WHERE val > 1 -- recursive query
)
SELECT *
FROM countdown;
```

Running this query will give you a table with 3 rows, each holding an integer value going from 3 at the first row to 1 at the last. You can change the `3` within the recursive CTE and get results with different number of rows. But what is this?!

Let's first understand two pieces of terminology:

1. **Initial (non-recursive) query:** The `SELECT` statement at the beginning of the recursive CTE is called the initial query.
2. **Recursive query:** The second `SELECT` statement after the `UNION` statement inside the recursive CTE is called the recursive query.

Let's now understand what is going on with the query above step by step.

### Step 1: Define the results and working tables

Running the query above will make Postgres create two temporary tables in the background; results table, and working table. These tables are going to be given some number of tables. The columns that they get assigned are whatever you have inside of the `( )` of the recursive CTE definition. In this example we have just one argument in the `( )`, therefor both of the temporary tables are going to get one single column labeled `val`.

### Step 2: Run the non-recursive statement

The non-recursive statement runs, and the results are put into the results table and working table. The non-recursive statement is a meaningful statement on its own, regardless of being used inside a recursive CTE. So you can run this query:

```sql
SELECT 3 AS val;
```

And you get back a table with one column called `val` and one row holding `3` as in integer value. So this is what the non-recursive statement does.

### Step 3: Run the recursive statement

In this step, the table name `countdown` is going to get replaced with a reference to the working table. Note that inside the recursive statement there is a `FROM countdown` clause. So instead of `countdown` you can say `WORKING_TABLE`. This is key to understand what is going on. So the recursive statement is basically doing this:

```sql
SELECT val - 1 FROM WORKING_TABLE WHERE val > 1;
```

> `WORKING_TABLE` is not an actual Postgres keyword. You cannot write it in your query. It is written here so you can understand the subject more easily.

While the working table holds `3` as `val` from step 2. So we are actually selecting all the different rows in the working table where `val` is greater than `1`. So as in intermediary result we now have `val - 1` which is `2`.

### Step 4: Append recursive statement's result to the results table and run recursion again

In this step, if the recursive statement returns some rows, Postgres will append them to the results table and run the recursion again. So now the results table has 1 column called `val` and 2 rows, first `3` and second `2` which is now appended to the table. Also, everything in the working table that is related to the previous steps is thrown away and replaced by the new value of the `val` parameter.

So as for the example that we are going through, we will now go back to step 3. We will then take `2` and the recursive statement will return `val - 1` which is `1` and this will again be stored in the results table and rapleced with the current value of `val` in the working table. In the next step, the recursive statement won't execute since `val` is no longer greater than `1`. So recursion will have no result.

IF the recursive statement returns no rows, Postgres will stop recursion. Finally, the results table will be called `countdown` and this table will be accessible to the rest of the query outside the recursive CTE. In the example, we are selecting everything from the final countdown table, and this is the result that we saw first.

| n   | val |
| --- | --- |
| 1   | 3   |
| 2   | 2   |
| 3   | 1   |

## When to use recursive CTE?

Let's go through another example, this time working on our real-world Instagram database. In the page where Instagram who the user should follow. Why are they recommended? Why should a user be recommended to follow some other users? Imagine you are following some people on your account. These are your first circle connections. Those people, in turn, follow some other people on their account. These are your second circle connections. So you are not directly connected to them. Instagram thinks you might be interested in following them too.

So for Instagram to come up with this list of suggestions, it would have to go over your list of followings, find who they are following in turn, and bring them up for you to follow. But there is something else to note here.

Imagine you, as a user, scroll down the suggestions list and find no interest in those people. So Instagram would have to continue suggest other people to you. It would probably have to go for your 3rd circle of connections. So Instagram would have to go over the following list of the second circle and suggest you to follow people at one step further in your circles. This is where you need a recursive CTE. You need a recursive CTE because people following other people, typically creates an undirectional graph structure. But as for the purpose of this Instagram feature example, we can assume that it is a directional tree, although it really is not.

Let's now write this query. Remember that we are trying to give some suggestions to a particular user. We are not generating suggestions for every person inside our application. Let's now find out which tables we are going to use to implement this feature in our database.

We probably only need the `users` and `followers` table. Let's imagine the process of finding some suggestions for the user ID of 1. The first thing we should do is to go through the followers table and find out who the targetted user is following. So we are going to find all the rows in the followers table where the `follower_id` is 1. We now have the ID of users that this user is following. They are users with IDs 4 and 5. Now we should find out who these two users are following. So we would have to go over the followers table again and find all the rows where `follower_id` is either 4 or 5. These will mark for us the users that we are going to suggest to the user ID 1 to follow them. As for the next circle, we would find who the second circle people are following and find them by going through the followers table again. You can now see why we call this recursive.

Let's now write the actual query. It is going to be challenging. Imagine we want to suggest people to the user with ID 1000.

```sql
WITH RECURSIVE suggestions(leader_id, follower_id, depth) AS (
		SELECT leader_id, follower_id, 1 AS depth -- non-recursive (start)
		FROM followers
		WHERE follower_id = 1000 -- non-recursive (end)
	UNION
		SELECT followers.leader_id, followers.follower_id, depth + 1 -- recursive (start)
		FROM followers
		JOIN suggestions on suggestions.leader_id = followers.follower_id
		WHERE depth < 3 -- recursive (end)
)
SELECT DISTINCT users.id, users.username
FROM suggestions
JOIN users ON users.id = suggestions.leader_id
WHERE depth > 1
LIMIT 30;
```

Pretty long one! Let's now go line by line over the query to fully understand it.

# Simplify queries with views

To understand views, let's start with an example: Show the most popular users - the users who were tagged the most.

We are going to need all the tags joined in one table, we don't need them separately as `captiong_tags` and `photo_tags` table. So would again use a `UNION`.

Then we are going to take a look at each individual user ID in the users table, then for each user, we are going to join it with each user in the joined tags table. You will end up with a table that for each username you will probably have multiple rows representing tags.

Afterwards, we could use a `GROUP BY` operation. We can group by username, and then count the number of rows for each different group. Then we could use a sorting operation to make sure we get the most popular users at the top of the table. This would be the SQL:

```sql
SELECT username, COUNT(*)
FROM users
JOIN (
	SELECT user_id FROM photo_tags
	UNION ALL
	SELECT user_id FROM caption_tags
) AS tags ON tags.user_id = users.id
GROUP BY username
ORDER BY COUNT(*) DESC;
```

The awkward part here is that, if you notice, we are doing this join again. Maybe this was a problem when we designed the database. Maybe we didn't need to create two separate tables for tags, and up until now we have not discovered any scenario where the separated tables would come in handy.

This section is all about fixing this kind of mistakes: when we have two tables and we are get the feeling that we actually need them two be one. There are 2 possible solutions.

## A possible solution

The first solution, which is not a good one is to merge the two tables into one table called `tags`. This would be the series of queries that we will use:

```sql
CREATE TABLE tags(
  id SERIAL PRIMARY KEY,
  create_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
  user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  post_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  x INTEGER,
  y INTEGER
);

INSERT INTO tags(created_at, updated_at, user_id, post_id, x, y)
SELECT created_at, updated_at, user_id, post_id, x, y
FROM photo_tags;

INSERT INTO tags(created_at, updated_at, user_id, post_id)
SELECT created_at, updated_at, user_id, post_id
FROM caption_tags;
```

But this solution is not a good one because of two reasons:

1. We cannot copy the IDs of photo_tags and caption_tags table to this new table since they must be unique. These tags might have ID numbers in common because they were stored in separate tables, and now they cannot be inserted into one table with their previoud IDs. So if there are other resources in the database that point to those IDs, then we are breaking the foreign keys referencing integrity.
2. If we delete the original tables that tags are coming from, we are actualy breaking existing queries that refer to those tables.

## A better solution: Create a view

Let's learn some important things about views:

1. A view is basically a fake table that has rows from other tables.
2. These can be exact rows as they exist on another table, or a computed value.
3. You can reference a view in any place where you would normally reference a table.
4. Views does not actually create a new table or move any data around.
5. Views don't have to be used for a `UNION`. They can compute absolutely any values.

You can think of a view as being very similar in nature to a common table expression (CTE). The only issue with a CTE is that you should attach them to some other query. On the other hand, with views, you can create them ahead of time and then refer to them at any future point in time in any other query.

Let's create a view:

```sql
CREATE VIEW tags AS (
	SELECT id, created_at, user_id, post_id, 'photo_tag' AS type FROM photo_tags
	UNION ALL
	SELECT id, created_at, user_id, post_id, 'caption_tag' AS type FROM caption_tags
);
```

Using `photo_tag` and `caption_tag` with the `AS type` clause, makes it possible for us to distinguish between a row that is related to a photo tag and a row related to a caption tag. No to refer to this view in another later query:

```sql
SELECT * FROM tags;
```

So this view is actually helping us solving the issue of having two separate tables where we actually need only one. you can remove this view anytime you want, and none of the original data will be lost from your database since they are stored in their own tables. So now the previous query:

```sql
SELECT username, COUNT(*)
FROM users
JOIN (
	SELECT user_id FROM photo_tags
	UNION ALL
	SELECT user_id FROM caption_tags
) AS tags ON tags.user_id = users.id
GROUP BY username
ORDER BY COUNT(*) DESC;
```

can be simplified and replaced with this query:

```sql
SELECT username, COUNT(*)
FROM users
JOIN tags ON tags.user_id = users.id
GROUP BY username
ORDER BY COUNT(*) DESC
```

## When to use a view

You saw one possible usecase of a view. Now let's go through another usecase and example. Here is a list of queries you might want to try.

The 10 most recent posts are really important:

- Show the users who created the 10 most recent posts
- Show the users who were tagged in the 10 most recent posts
- Show the average number of hastags used in the 10 most recent posts
- Show the number of likes each of the 10 most recent posts received
- Show the hashtags used by the 10 most recent posts
- Show the total number of comments the 10 most recent posts received

Notice that in all these different queries how the ten most recent posts are very important and needed. For each, we could write a subquery that would find us the 10 top most recent posts. Alternatively, putting the concept of views in use, we could create a view that finds the 10 most recent posts. We could then reuse that view in all the queries above, without having to rewrite the logic again and again.

Let's now create a view to find the 10 most recent posts:

```sql
CREATE VIEW recent_posts AS (
	SELECT *
	FROM posts
	ORDER BY created_at DESC
	LIMIT 10
);
```

You can now refer to this view whenever you want:

```sql
SELECT * FROM recent_posts;
```

We can now use this view in the queries above. For instnace, for the first query:

```sql
SELECT username FROM recent_posts
JOIN users ON users.id = recent_posts.user_id;
```

## Deleting and changing a view

Remember the last view we created: It contained the 10 most recent posts. Maybe in a new requirement, we would now need to get the 15 most recent posts. So we now need to change the configuration of the view.

```sql
CREATE OR REPLACE VIEW recent_posts AS (
	SELECT *
	FROM posts
	ORDER BY created_at DESC
	LIMIT 15
);
```

To delete a view you can use this query:

```sql
DROP VIEW recent_posts;
```

You can no longer refer to this view afterwards.

# Optimizing queries with materialized view

So now you know that a view is a query that gets executed every time you refer to it. But let's now understand what a materialized view is. A materialized view is a query that gets executed only at very specific times, but the results are saved and can be referenced without re-running the query.

We use materialized views anytime that we have a very expensive query; that is, a query that might take seconds, minutes, or even hours to execute. We can run a materialized view just one time, hang on to the result set, and refer to the results without having to re-run the expensive query.

There is a similarity between views and CTEs. Just as simple CTEs were convenience tools compared to recursive CTEs, simple views are the same compared to materialized views. So materialized views add major functionality like recursive CTEs.

To understand materialized views better, let's go over an example: For each week, show the number of likes that posts and comments received. Use the post and comment created_at date, not when the like was received.

This query takes a considerable amount of time to execute, so it is a good case for materialized view.
We are going to use our tables of posts, comments, and likes. Remember that likes have a `post_id` and a `comment_id`. For each like, only one of the columns have an ID value and the other would definitely be `null`. This is important, because we will use this fact when we are going to write the query.

We should first group the whole lifetime of the application by weeks. Then, we should assign posts and comments to these week groups based on their `created_at` value. Then we would have to find out how many likes each post and comment has received.

As for the stage of writing the query, we are first going to join the three tables of likes, posts, and comments together. So, likes with posts, and then liskes with comments. But we need a reminder here. Do you remember _left join_? We are going to use it here. If we go for a normal _inner join_ between likes and posts, we would lose the rows in likes that have null values for `post_id`. So instead of an inner join, we use a left join. This will make the query keep all the rows from the likes table no matter what the value of the `post_id` is. This would be the diagram of the final joined table:

![sql-indexes-14](/images/sql/sql-indexes-14.jpg)

> Using a normal inner join would return with a complete empty table. This is a sign that you are probably using the wrong type of join.

## A slow query

Let's first write the query for the 3-way join we explained previously. This query will take a noticable amount of time by itself.

```sql
SELECT *
FROM likes
LEFT JOIN posts ON posts.id = likes.post_id
LEFT JOIN comments ON comments.id = likes.comment_id;
-- returns a table just like the figure above
```

Next, we are going to take the `created_at` column of either a post or a comment in this joined table, and somehow round them to weeks. This is only achievable by using a Postgres built-in function that truncates date values. This function allows you to pull one piece of information out of a timestamp. In this case we are going to pull out week. Let's see how you can use it here:

```sql
SELECT
	date_trunc('week', COALESCE(posts.created_at, comments.created_at))
FROM likes
LEFT JOIN posts ON posts.id = likes.post_id
LEFT JOIN comments ON comments.id = likes.comment_id;
```

So the `date_trunc('week', COALESCE(posts.created_at, comments.created_at))` line is actually passing two arguments to the `date_trunc` function. The first argument is the unit to which the date values are going to be rounded. We are then going to pass the `created_at` value of either a post or a comment; one that is not null. So we use the `COALESCE` clause and pass both `created_at` values to it and let it find out which one is not null and give it to the `date_trunc` function.

Don't forget to give this line an alias in the query:

```sql
SELECT
	date_trunc('week', COALESCE(posts.created_at, comments.created_at)) AS week
FROM likes
LEFT JOIN posts ON posts.id = likes.post_id
LEFT JOIN comments ON comments.id = likes.comment_id;
```

This is a piece of the table that you get back:

```
--week--
"2015-08-03 00:00:00+04:30"
"2013-07-08 00:00:00+04:30"
"2016-05-09 00:00:00+04:30"
"2016-08-08 00:00:00+04:30"
"2017-01-02 00:00:00+03:30"
"2013-12-09 00:00:00+03:30"
"2016-10-24 00:00:00+03:30"
...
```

Notice that in all the rows, the time value is now `00:00:00`. The dates are now divided into weeks but we cannot be sure now, we cannot see it. So let's sort this table by using an `ORDER BY` at the end:

```sql
SELECT
	date_trunc('week', COALESCE(posts.created_at, comments.created_at)) AS week
FROM likes
LEFT JOIN posts ON posts.id = likes.post_id
LEFT JOIN comments ON comments.id = likes.comment_id
ORDER BY week;
```

You now see the returned table as:

```
--week--
"2010-01-11 00:00:00+03:30"
"2010-01-11 00:00:00+03:30"
"2010-01-11 00:00:00+03:30"
"2010-01-11 00:00:00+03:30"
"2010-01-11 00:00:00+03:30"
"2010-01-11 00:00:00+03:30"
"2010-01-11 00:00:00+03:30"
"2010-01-18 00:00:00+03:30"
"2010-01-18 00:00:00+03:30"
"2010-01-18 00:00:00+03:30"
"2010-01-18 00:00:00+03:30"
"2010-01-18 00:00:00+03:30"
"2010-01-18 00:00:00+03:30"
...
```

As you see we have multiple rows for the data related to each week. But we would like to group the rows by the weeks and then put some `COUNT` at work.

```sql
SELECT
	date_trunc('week', COALESCE(posts.created_at, comments.created_at)) AS week,
	COUNT(posts.id) AS num_posts,
	COUNT(comments.id) AS num_comments
FROM likes
LEFT JOIN posts ON posts.id = likes.post_id
LEFT JOIN comments ON comments.id = likes.comment_id
GROUP BY week
ORDER BY week;
```

Results are:

```
--week--                       --num_posts-- --num_comments--
"2010-01-11 00:00:00+03:30"	        29	             0
"2010-01-18 00:00:00+03:30"	        31	             3
"2010-01-25 00:00:00+03:30"	        34	             0
"2010-02-08 00:00:00+03:30"	        29	             10
"2010-02-22 00:00:00+03:30"	        84	             13
"2010-03-01 00:00:00+03:30"	        33	             0
"2010-03-08 00:00:00+03:30"	        23	             0
"2010-03-15 00:00:00+03:30"	        81	             0
...
```

Writing this query is a pretty nice exercise. But as we said earlier, this is a slow query. Let's now find out how to use materialized views to make it fast.

## Better solution

The better solution is to create a materialized view out of the query we wrote previously. So we are going to use the materialized view to only run the query at very specific times. After the query is executed, Postgres will hold on to the result set, and then we can refer back to those results anytime we want. This will cause a hige performance improvement.

```sql
CREATE MATERIALIZED VIEW weekly_likes AS (
SELECT
	date_trunc('week', COALESCE(posts.created_at, comments.created_at)) AS week,
	COUNT(posts.id) AS num_posts,
	COUNT(comments.id) AS num_comments
FROM likes
LEFT JOIN posts ON posts.id = likes.post_id
LEFT JOIN comments ON comments.id = likes.comment_id
GROUP BY week
ORDER BY week
) WITH DATA;
```

Running this query for the first time will take a considerable amount of time and Potgres will respond:

```
SELECT 518
Query returned successfully in 959 msec.
```

We can now refer to the result of this materialized view with its name `weekly_likes` and run any query on those results.

```sql
SELECT * FROM weekly_likes;
```

Now this query runs pretty fast because results are being read from cache. It does not take long anymore.

The downside of materialized view is that if any change is introduced to the likes or posts tables, it will not modify the materialized view cached results. In this situation, we would have to manually tell Postgres to refresh the materialized view. How?

```sql
REFRESH MATERIALIZED VIEW weekly_likes;
```

# Managing database design with schema migrations

Schema migration is all about making very careful and well-planned changes to the structure of a database; that is, adding columns to a table, removing columns, renaming them, adding tables and removing them and so on.

Schema migration is very important especially when you work with a team. To understand what schema migration really is, we will go through a really busy day you might have working on a database. We will identify some huge issues that come up when you start changing the structure of your database, implementing them directly on the database itself. These are issues that you will certainly run into at some point.

The story is about a database engineer working on the comments table in all our previous examples. You have a copy of the database on your local machine, and the original database runs on an AWS server. Whenever you want to do something, you try it on the database copy on your local machine, and then move to the original database.

Now let's say that you have received a feedback from the developers team saying that the 'contents' column name in the comments table is not good and you have to rename this column. Implementing this change just on the table itself is not a hard thing to do:

```sql
ALTER TABLE comments
RENAME COLUMN contents TO body;
```

So you would do this first on your local machine, and you are not moving any data around. You may think that everything looks good and you would continue implementing this on the original database. Once you run the statement above on the production database, you will get yourself into trouble. Why?

As an application, you will have requests arriving at your API server. The API will try to extract some comment text from the request, use that text to build some kind of SQL statement to insert or create a comment, and then send it to the database. Here is the issue: you renamed the `contents` column to `body`, but you didn't update your API server to match this change. So your API will probably still continue creating SQL statements that include the `contents` column name. So postgres will return errors.

Here is the first big lesson: **Changes to the database structure and changes to clients need to be made at preceisely the same time.**

But even if you start database and API new version deployments at the same time, you might very well still encounter critical errors. Because, the time that takes for the database to deploy might be very shorter from the time that it takes for the API to deploy. During this window gap, you will still have requests and find yourself in the middle of thousands of errors, because while the database new version is deployed, the API new deployment is still in progress, and the currently working version of the API is still refering to the `contents` column, but the new database deployment expects `body`.

The solution to this situation is something that you might have encountered many times with different applications. They usually announce that they are going to deactivate the whole application for a certain amount of time to do some changes. This is exactly when they are implementing this kind of change to their database and API. But some services and applications cannot temporarily be turned off. They should always work. For these scenarios, you cannot have that window gap.

Here is the second big lesson: **When working with other engineers, we need a really easy way to tie the strucutre of our database to our code.**

It is now time to understand how schema migrations solve this issue.

## Migration files

Previously, we tried to implement the database update by openning up the PGAdmin and implementing the change there by running some commands that alter the strucutre of the database. But as we learned, this is not a good way. Instead, we are going to author something called a schema migration file. Schema migration files are files that contain some amount of code that describe a very precise and detailed change that we want to make to our database. For instance, in the previous example, the migration file would contain some code that says we want to rename the `contents` column to `body`.

But what is inside this migration file really? A migration file can be written in any programming language. In general, a schema migration file contains 2 different sections: section **Up** (upgrade) and section **Down** (downgrade). So the up section contains a statement that advances, or upgrades the structure of the database. The down section contains a statement that exactly undo's the up secton command.

![sql-indexes-15](/images/sql/sql-indexes-15.jpg)

Once you author a migration file, you then **apply** it to the database. This way you have thought of a way to **revert** back the change you made in the up section. A project might include several migration files during its whole working time.

## Issues solved by migrations

As for the first important lesson we learned previously, using migration files, updating the database and API of an application can be done very easily, following this precedure over time:

![sql-indexes-16](/images/sql/sql-indexes-16.jpg)

> Some other issues still exists in this flow. We will take care of them later...

As for the second lesson we can do it with migration files very easily.

![sql-indexes-17](/images/sql/sql-indexes-17.jpg)

## Writing migration files

There are multiple libraries for implementing migration files. Here is a list for JavaScript:

1. `node-pg-migrate`
2. `typeorm`
3. `sequelize`
4. `db-migrate`

Many migration tools can automatically generate migration files for you. However, it is highly recommended that you write all migrations manually using plain SQL. This is mainly because most of the libraries, make some intrinsic assumtions around some options that might want to apply to some column. For instance, they might apply some default values.

In this example, we are going to use `node-pg-migrate`, but we are actually going to write plain SQL using this library.

## Practical example

We first need to create a sample project with Node.

```
npm init -y
```

This will create a `package.json` file. Then install two packages, one for migration, and one for connecting to Postgres database:

```
npm install node-pg-migrate pg
```

We are now going to create a new database in PGAdmin called "socialnetwork". Before creating a migration file, we need to add a script to the `scripts` property of the package json file of the project. Go on and remove the `test` script, and write this one instead:

```json
"scripts": {
  "migration": "node-pg-migrate"
}
```

This script will help us access the `node-pg-migrate` CLI from the terminal.

As our first migration file, we are going to use this command in the terminal:

```
npm run migrate create table comments
```

This will generate a file in our project, placed inside a `migrations` directory. The file is named with a timestamp that refers to this migration file's time of creation. This timestamp is tracked as multiple migration files would have to be executed in the correct order in time.

This is the initial strucutre of the generated migration file:

```js
/**
 * @type {import('node-pg-migrate').ColumnDefinitions | undefined}
 */
exports.shorthands = undefined;

/**
 * @param pgm {import('node-pg-migrate').MigrationBuilder}
 * @param run {() => void | undefined}
 * @returns {Promise<void> | void}
 */
exports.up = (pgm) => {};

/**
 * @param pgm {import('node-pg-migrate').MigrationBuilder}
 * @param run {() => void | undefined}
 * @returns {Promise<void> | void}
 */
exports.down = (pgm) => {};
```

And we are going to write our migration file as:

```js
/**
 * @type {import('node-pg-migrate').ColumnDefinitions | undefined}
 */
exports.shorthands = undefined;

/**
 * @param pgm {import('node-pg-migrate').MigrationBuilder}
 * @param run {() => void | undefined}
 * @returns {Promise<void> | void}
 */
exports.up = (pgm) => {
  pgm.sql(`
        CREATE TABLE comments(
            id SERIAL PRIMARY KEY,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            udpated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            contents VARCHAR(240) NOT NULL
        );
        `);
};

/**
 * @param pgm {import('node-pg-migrate').MigrationBuilder}
 * @param run {() => void | undefined}
 * @returns {Promise<void> | void}
 */
exports.down = (pgm) => {
  pgm.sql(`
        DROP TABLE comments;
        `);
};
```

Now to apply this migration, you would have to use this command in Windows Git Bash:

```
DATABASE_URL=postgres://USERNAME:PASSWORD@localhost:5432/socialnetwork npm run migrate up
```

To revert the migration:

```
DATABASE_URL=postgres://USERNAME:PASSWORD@localhost:5432/socialnetwork npm run migrate down
```

You can check the result of applying and reverting the migration in PGAdmin.

Now if you create another migration file and do something different as a second step, then when you use the terminal command to run the up section of your migrations, it will automatically start with the first migration file and then continue to the second. If the first one is already done from before, it will skip the first and continue to the second. However, if you try to revert the migration down, it will go back only one step, reverting only the last migration. If you want to revert one step further, you will have to run the down command again.

In all the discussion during this section, we talked about updating or changing the structure of our database, and not the data itself. For instance, you may want to merge two columns into one new column, or change the data type of a column. This would require you to apply some changes to the data itself along the migration. How would you do this?

Also, you might want examine if this process of applying migrations can help you avoid any downtimes on your application. Does it help?

# Schema vs. data migration

Let's first imagine a situation. You have a posts table which holds `id`, `url`, `lat`, and `lng` columns. Years after the database and application is created and has been working, you decide for some reason to merge `lat` and `lng` columns into one `loc` column. But until then, you might already have thousands of rows inside the current version of the table. So you need to find a way to take all the already existing data in your table and merge them into one single value for the new `loc` column.

You need to devise a strategy on how to merge the data of these two columns together. This could be an algorithm: First, add the column `loc` to the table. Second, copy `lat` and `lng` to `loc`. Finally, drop `lat` and `lng` columns. It is interesting to know that the first and the last step is called **Schema migration** as we observed in the prevsious section. The second step, which is a bit more complicated, is called **Data migration**. This second step is not about changing the structure of the database. It is about moving data around between different columns.

## Dangers around data migrations

There are several reasons not to run data migrations at the same time as schema migrations. Let's focus on one of the reasons now. You mainly need to decide between two ways:

1. Doing the whole process of schema and data migration in one single migration file: This could very easily get us into a huge trouble. First step which is adding a column is quick. Second step would take a really long time if you already have millions of records in your table. Then the final step is quick.

Whenever we run a migration, it is common to place the migration inside of a **transaction**. This means that if some error happens, for example, in the final step of the migration, we don't want the migration in a kind of half-executed state. So if some error happens during the migration process, we want the whole process to go back. That is why we usually execute a migration process in a transaction. There are also other cases when you don't run migrations in transactions, but they are very rare.

Whenever you open up a transaction, you can imagine that we are opening a separate workspace where you are copying all of your data into it. It is essentially a snapshot of the last state of your table. This is what you can imagine to understand the situation better. It is not what really happens behind the scenes. After copying all your data into this new imaginary workspace, you do some work on it, and then if there are no errors, you commit the transaction. Commiting the transaction will merge back all the changes to the main process. Remember that the execution of the transaction can take a lot of time for moving data around, and while this process is going on, your application is online and you are still accepting requests. So while the transaction is working on the last snapshot of your table, your table is accepting new records in its current structure, not the updated structure. These rows that were added online to your table, will not be included in the transaction.

So following the example we mentioned, the `loc` column is being filled with the formatted data of `lat` and `lng` columns coming from the table snapshot that was created at the beginning of the transaction, and at the same time new rows are being inserted into your table through your online application and API. These new rows are not included in the snapshot being used in the transaction, and therefore the new `loc` column for these additional rows will end up having `NULL` value.This means that you have lost the location coordinates related to these posts!

This was just one reason not to run data migrations and schema migrations at the same time. There are several other reasons.

## Properly running data and schema migrations

2. Split the three steps into three separate migration file, and allow some amount of time to pass between each step. For the first step, we do a schema migration by creating a column called `loc` and set it to allow us to put `NULL` value in it for every row. Eventually, we will set the non-null constraint on this column, but not now. After this step and before the second step of migration, you would have to deploy the new version of API that will write values to both `lat`/`lng` and `loc` columns. Between this and the previous step you could have some days off. The specific quality of this whole approach is that it is not time-dependant. After this step, our API will continue receiving requests and all the newly inserted rows will have values for the 3 columns. But the rows that were inserted before, will have `NULL` in their `loc` column. In the next step, which is originally the second step of migration, we go for updating the rows that were in the table from before and are not holding `NULL` for `loc`. We will copy their `lat` and `lng` values to their `loc` column. Before the final migration step, we would update the API code to only write to the `loc` column and leave `lat` and `lng` columns `NULL`. Finally, at the last migration step, since the application is no longer recording values into `lat` and `lng` columns, we will eventually drop them.

## Let's run migrations in practice

### Running an Express server with database connection

First, we need to setup a Node server application using Express library, and then, establish a database connection within the server using the `pg` NPM package. We finally try the connection to see if it is set up right.

```js
const express = require("express");
const pg = require("pg");

const pool = new pg.Pool({
  host: "localhost",
  port: 5432,
  database: "socialnetwork",
  user: "postgres",
  password: "hotButter",
});

pool.query("SELECT 1 + 1;").then((res) => console.log(res));
```

Let's now implement an express app and a middleware to allow us to receive form submissions through browser. Afterwards, we are going to set up a route handler for the GET requests on the `/posts` url.

```js
const app = express();

app.use(express.urlencoded({ extended: true }));
// This is a middleware that allows us to receive form submissions through a browser

// ROUTE HANDLERS:
app.get("/posts", async (req, res) => {
  const { rows } = await pool.query(`
        SELECT * FROM posts;
        `);

  res.send(`
            <table>
                <thead>
                    <tr>
                        <th>id</th>
                        <th>lng</th>
                        <th>lat</th>
                    </tr>
                </thead>
                <tbody>
                    ${rows
                      .map((row) => {
                        return `
                            <tr>
                                <td>${row.id}</td>
                                <td>${row.lng}</td>
                                <td>${row.lat}</td>
                            </tr>
                        `;
                      })
                      .join("")}
                </tbody>
            </table>
            <form>
                      <h3>Create post</h3>
                      <div>
                        <label>Lng</label>
                        <input name="lng" />
                      </div>
                      <div>
                        <label>Lat</label>
                        <input name="lat" />
                      </div>
                      <button type="submit">Create</button>
            </form>
            `);
});

app.listen(3005, () => {
  console.log("listening on port 3005");
});
```

> Don't forget to make the app keep listening for incoming requests on a specific port.

Then implement a POST request for `/posts` url and

```js
app.post("/posts", async (req, res) => {
  const { lng, lat } = req.body;

  await pool.query(
    `
            INSERT INTO posts (lat, lng)
            VALUES ($1, $2);
        `,
    [lat, lng]
  );

  res.redirect("/posts");
});

app.listen(3005, () => {
  console.log("listening on port 3005");
});
```

You should now be able to submit some data throught the form and your database will store that data and you server application will show it to you on the HTML page.

### Adding the `loc` column

First, in the terminal and at the root of your project run this command to create a migration file:

```
npm run migrate create add loc to posts
```

Then go to the file in the code editor and implement the migration:

```js
/**
 * @type {import('node-pg-migrate').ColumnDefinitions | undefined}
 */
exports.shorthands = undefined;

/**
 * @param pgm {import('node-pg-migrate').MigrationBuilder}
 * @param run {() => void | undefined}
 * @returns {Promise<void> | void}
 */
exports.up = (pgm) => {
  pgm.sql(`
            ALTER TABLE posts
            ADD COLUMN loc POINT;
        `);
};

/**
 * @param pgm {import('node-pg-migrate').MigrationBuilder}
 * @param run {() => void | undefined}
 * @returns {Promise<void> | void}
 */
exports.down = (pgm) => {
  pgm.sql(`
        ALTER TABLE posts
        DROP COLUMN loc;
        `);
};
```

Then run the migration on your project terminal using this command:

```
DATABASE_URL=postgres://postgres:hotButter@localhost:5432/socialnetwork npm run migrate up
```

Then run the server again. At this point in time, your posts table is upgraded with a `loc` column, but the server application will still keep submitting values for `lat` and `lng` and your database will keep receiving them, leaving the `loc` column `NULL` for new records.

### Updating server code

So as the next step, we are now going to update the server code to submit data for both `lat`/`lng` and `loc` columns. So actually, you just need alter the POST route handler.

```js
app.post("/posts", async (req, res) => {
  const { lng, lat } = req.body;

  await pool.query(
    `INSERT INTO posts (lat, lng, loc)
        VALUES ($1, $2, $3);`,
    [lat, lng, `(${lng}, ${lat})`]
  );

  res.redirect("/posts");
});
```

Now you can go to the application again, submit some data and you will see that your database is now filled with data for both `lat`/`lng` and `loc` columns for the newly inserted record.

### Data migration for previous data

For the next step, we need to update the data that was already existing in our database, and fill their `loc` column with proper data. For this step, we have 2 solutions.

1. Determine updates in JS. This means that you are going to put some instructions in a JS file where you first, query for all the rows in the table where `loc` is `NULL`. Then you run some busines logic and some validation in JavaScript. Finally, you send the updated data to the database and update the targetted records with the new data format.

![sql-indexes-18](/images/sql/sql-indexes-18.jpg)

This approach has a couple of downsides to it:

- The query that we use to acquire all the records where `loc` is null might return with millions of records and the Node server might crash. You could use a technique called **Batching** to load only a limited set of records using the `LIMIT` clause in SQL
- Batching could also fail halfway and leave us with the process incomplete. For instance, at the last batch, there could be some error in our logic in how we update or calculate the updated data. This will make it so that some records of the table are updated and some others are not. We could also solve this by using a transaction but that also has a couple of challenges.
- This method requires us to manually connect to the database from a Node environment. Connecting to a database hosted on some services using JavaScript files might be a little bit challenging.

The very big upsade to this approach is:

- You can very easily run really complex business logic or do some kind of complex validation on all the records before you process the update. Running validations in SQL are pretty hard.

2. Rely just upon SQL. You could write a simple JavaScript file that connects to the database and just send some update statement to the database. You could also write the statement directly inside PGAdmin. The upside to this approach is:

- There is no moving data between database and Node application, which makes the whole process as fast as possible.

However, the downside is:

- It is a lot harder to implement validation or business logic.

> There is one issue with which both options 1 and 2 are troubled. This happens when there are many many records inside the table. We are talking about hundreds of thousands or millions of records. With either approach, you might want to run the entire update inside a single transaction. There is a good reason for doing that; that is, if something goes wrong at the very last row, it means that there might be a bug in your updating logic. If this happens, none of the updates will be saved to the database and the transaction would be undone. But there is a little issue around a long-running transaction that you need to be aware of.
>
> Imagine your transaction is running and it has just finished processing a particular record. That particular record will be locked by the transaction. This row cannot be updated until either the transaction has fully commited or completely rolled back. During the locked time, if a user sends a request to the application and tries to update the record that is being locked, they will fail or they would have to wait until the migration transaction is done. Their transaction would not be allowed to run until the migration transaction finishes processing.

#### Implementing the data transaction

We are going to work with option 2 from the section above, and we are going to create a JS file in our server application. This JS file should be placed inside a `data` directory within the `migration` directory. This way, the `node-pg-migrate` module will only look into the `migrations` directory. Putting this file inside the `data` directory makes the module understand that this is a data migration file. The JS file name should start with `01` for the first data migration file. This number will provide the module with an index, so that it would know the order by which the data migration files should be executed.

Inside this file, we are going to connect to the database and run a query to update the posts table:

```js
const pg = require("pg");

const pool = new pg.Pool({
  host: "localhost",
  port: 5432,
  database: "socialnetwork",
  user: "postgres",
  password: "hotButter",
});

pool.query(`
    UPDATE posts
    SET loc = POINT(lng, lat)
    WHERE loc IS NULL;
    `);
```

This query returns a promise to which you can attach a `then` handler.

```js
const pg = require("pg");

const pool = new pg.Pool({
  host: "localhost",
  port: 5432,
  database: "socialnetwork",
  user: "postgres",
  password: "hotButter",
});

pool
  .query(
    `
    UPDATE posts
    SET loc = POINT(lng, lat)
    WHERE loc IS NULL;
    `
  )
  .then(() => {
    console.log("Update complete");
  })
  .catch((err) => console.error(err.message));
```

You can then move into the `migrations/data` directory in the terminal and run the data migration file called `01-lng-lat-to-loc.js`.

```
<!-- @ migrations/data -->
node 01-lng-lat-to-loc.js
```

You can now go and confirm the changes in PGAdmin.

### Updating server code (again, to now only generate `loc` data)

From this point on, our server application will no longer store data in `lat` and `lng` columns. They will simply be left `null`.

```js
const express = require("express");
const pg = require("pg");

const pool = new pg.Pool({
  host: "localhost",
  port: 5432,
  database: "socialnetwork",
  user: "postgres",
  password: "hotButter",
});

const app = express();

app.use(express.urlencoded({ extended: true }));

// ROUTE HANDLERS:
app.get("/posts", async (req, res) => {
  const { rows } = await pool.query(`
        SELECT * FROM posts;
        `);

  res.send(`
            <table>
                <thead>
                    <tr>
                        <th>id</th>
                        <th>lng</th>
                        <th>lat</th>
                    </tr>
                </thead>
                <tbody>
                    ${rows
                      .map((row) => {
                        return `
                            <tr>
                                <td>${row.id}</td>
                                <td>${row.loc.x}</td>
                                <td>${row.loc.y}</td>
                            </tr>
                        `;
                      })
                      .join("")}
                </tbody>
            </table>
            <form method="POST">
                      <h3>Create post</h3>
                      <div>
                        <label>Lng</label>
                        <input name="lng" />
                      </div>
                      <div>
                        <label>Lat</label>
                        <input name="lat" />
                      </div>
                      <button type="submit">Create</button>
            </form>
            `);
});

app.post("/posts", async (req, res) => {
  const { lng, lat } = req.body;

  await pool.query(
    `INSERT INTO posts (loc)
        VALUES ($1);`,
    [`(${lng}, ${lat})`]
  );

  res.redirect("/posts");
});

app.listen(3005, () => {
  console.log("listening on port 3005");
});
```

### Droping `lat` and `lng` columns

In this step, we are going to need another schema migration file. So in the terminal:

```
npm run migrate create drop lng and lat from posts
```

This will, again, create a migration file in the `migrations` directory and we fill it like this:

```js
/**
 * @type {import('node-pg-migrate').ColumnDefinitions | undefined}
 */
exports.shorthands = undefined;

/**
 * @param pgm {import('node-pg-migrate').MigrationBuilder}
 * @param run {() => void | undefined}
 * @returns {Promise<void> | void}
 */
exports.up = (pgm) => {
  pgm.sql(`
        ALTER TABLE posts
        DROP COLUMN lat,
        DROP COLUMN lng;
        `);
};

/**
 * @param pgm {import('node-pg-migrate').MigrationBuilder}
 * @param run {() => void | undefined}
 * @returns {Promise<void> | void}
 */
exports.down = (pgm) => {
  pgm.sql(`
        ALTER TABLE posts
        ADD COLUMN lat numeric,
        ADD COLUMN lng numeric;
        `);
};
```

Then run the migration using this command in the terminal:

```
DATABASE_URL=postgres://postgres:hotButter@localhost:5432/socialnetwork npm run migrate up
```

Now you can run the Node server again and everything is fine.

# Accessing Postgres from APIs

We are now going to learn how you should connect to your Postgres database from a Node API. Along this entire section, we are really focused on how we work with a database from a real application.

Let's basically setup a Node API application using the terminal:

```
<!-- @Desktop -->
mkdir api

cd api

mkdir social-repo

cd social-repo

<!-- @social-repo -->
npm init -y

npm install dedent express jest node-pg-migrate nodemon pg pg-format supertest
```

Now open your code editor and let's edit the `scripts` inside the package json file.

```json
{
  "scripts": {
    "migrate": "node-pg-migrate",
    "start": "nodemon index.js"
  }
}
```

Next, we are going to generate a new migration file to create the users table inside the social network database.

```
npm run migrate create add users table
```

Then go on and implement the migration process:

```js
/**
 * @type {import('node-pg-migrate').ColumnDefinitions | undefined}
 */
exports.shorthands = undefined;

/**
 * @param pgm {import('node-pg-migrate').MigrationBuilder}
 * @param run {() => void | undefined}
 * @returns {Promise<void> | void}
 */
exports.up = (pgm) => {
  pgm.sql(`
        CREATE TABLE users (
            id SERIAL PRIMARY KEY,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            bio VARCHAR(400),
            username VARCHAR(30) NOT NULL
        );
        `);
};

/**
 * @param pgm {import('node-pg-migrate').MigrationBuilder}
 * @param run {() => void | undefined}
 * @returns {Promise<void> | void}
 */
exports.down = (pgm) => {
  pgm.sql(`
        DROP TABLE users;
        `);
};
```

We are using a fresh database. So we have a `socialnetwork` database, but there is no table in it. Then run the migration file:

```
DATABASE_URL=postgres://postgres:hotButter@localhost:5432/socialnetwork npm run migrate up
```

This was a quick basic setup for the Node API that we are going to develop right now.

## Building the API with users router

Here is a list of routes and methods that we are going to implement in the Node API:

![sql-indexes-19](/images/sql/sql-indexes-19.jpg)

### Creating the server in `app.js`

To start implementing the application, we create a folder called `src` at the root of our project, then create a file called `app.js` in it. Within this file, you are going export a function that returns the `app` created by calling Express.

```js
// app.js
const express = require("express");

module.exports = () => {
  const app = express();
  app.use(express.json());
  return app;
};
```

This may be a little different to what you are usually used to do with Express applications. You were probably expecting to create an app by calling Express, then add some middleware and finally make the app listen to requests on a specific port number. But we are taking a different approach here, for which there is really good reason: Our testing strategy.

### Creating the routes file

Next, we are going to create a routes file. This file will contain all the routes definitions tied to the users resource. So inside the `src` directory, we are going to create a folder called `routes`, and in it, we are going to create a `users.js` file and set it up like this:

```js
// users.js
const express = require("express");
const router = express.Router();
```

Then go on and define your routes and finally export the router.

```js
// users.js
const express = require("express");
const router = express.Router();

router.get("/users", async (req, res) => {});

router.get("/users/:id", async (req, res) => {});

router.post("/users", async (req, res) => {});

router.put("/users/:id", async (req, res) => {});

router.delete("/users/:id", async (req, res) => {});

module.exports = router;
```

We are then going to import this router in the `app.js` file and associate the router with the express application.

```js
// app.js
const express = require("express");
const usersRouter = require("./routes/users");

module.exports = () => {
  const app = express();
  app.use(express.json());
  app.use(usersRouter);
  return app;
};
```

### Establishing connection with database

To establish a connection between the Express application and the Postgres database making us able to run queries on the database, we are going to use the `pg` module. It is good to know that the `pg` module does not do any query building, it does not do any validation, it does not do any checking, it is nothing but to run some SQL. It is a very popular module in the Node community. This module can also be used to build a client. We usually don't use a client directly, because when you create a client, it can only execute one query at a time. This is a very big issue when building APIs. Because if we ever get multiple requests arriving at our API and we need to run multiple queries at the same time, with a client we would only be able to run one of those queries at a time. So rather than using a client directly, we generally use something called a **Pool**.

A pool maintains a list of several different clients. Then anytime that you need to run a query, you ask the pool to run a query for you. The pool will take your query, hand it off to one of the clients that is free internally, and that client will execute the query in Postgres.

> There is one scenario in which you use a client directly. That is, when you need to write or run a transaction. However, using a client or a pool is pretty similar.

We are now going to create a new file called `pool.js` in the `src` directory, and we are going to make a pool inside it. Just a little point here. You would normally create and export a pool like this:

```js
// pool.js
const pg = require("pg");

const pool = new pg.Pool({
  host: "localhost",
  port: 5432,
});

module.exports = pool;
```

However, on this example, we are not going to do this for an extremely important reason. Again, it is related to the testing stuff that we are going to implement later. If you create a pool like this, it would make it extremely challenging to connect to multiple databases. Why would you want to connect to several databases? When you implement and run tests, you would like to be able to do it on one single application that can connect to different databases.

Instead, we are going to create a pool and wrap it up inside a class.

```js
const pg = require("pg");

class Pool {
  _pool = null;

  connect(options) {
    this._pool = new pg.Pool(options);
  }
}

module.exports = new Pool();
```

This approach allows us to tell our pool some time in the future to connect to a different database. Let's continue setting up this pool.

### Start the Express app and using the pool

We are now going to create a file called `index.js` at the root of our project. In this file, we are going to start the Express app, and tell our pool to connect to our local database. So first, go on and require the `app` and `pool` modules.

```js
// index.js
const app = require("./src/app");
const pool = require("./src/pool");
```

Then tell your app to listen for traffic, and your pool to connect to some given atabase. Make sure you make the database connection first, because we want to first check for any invalid credentials regarding the database connection, and if there are errors, we don't want our application to ever start working. Let's go through this setup which you might think will work fine:

```js
// index.js
const app = require("./src/app");
const pool = require("./src/pool");

pool.connect({
  host: "localhost",
  port: 5432,
  database: "socialnetwork",
  user: "postgres",
  password: "hotButter",
});

app().listen(3005, () => {
  console.log("Listening on port 3005");
});
```

But it does not. Creating a pool does not establish a connection with the database. It is only when a client inside the pool is created to run a query that a connection is made. So creating the pool first and then running the app on a specific port will not respond with errors if database connection credentials are invalid. So, in addition to creating the pool, we should somehow tell the pool to conenct to the database and check the credentials. The easiest way to do that is to run a simple query. So you would want to go back to the `pool.js` file and add a simple query to the `connect` function defined inside the class:

```js
// pool.js
const pg = require("pg");

class Pool {
  _pool = null;

  connect(options) {
    this._pool = new pg.Pool(options);
    return this._pool.query("SELECT 1 + 1;");
    // this query will make the pool check the connection credentials. It is a common strategy used by many libraries.
  }
}

module.exports = new Pool();
```

The query that we added to the `connect` method, will return a promise. This promise will only resolve if the query inside it runs successfully. So we would have to update the code inside `index.js` to account for this promise:

```js
// index.js
const app = require("./src/app");
const pool = require("./src/pool");

pool
  .connect({
    host: "localhost",
    port: 5432,
    database: "socialnetwork",
    user: "postgres",
    password: "hotButter",
  })
  .then(() => {
    app().listen(3005, () => {
      console.log("Listening on port 3005");
    });
  })
  .catch((err) => {
    console.error(err);
  });
```

So the app would only start to listen if the connection is established successfully. The `catch` phrase will also be able to receive any errors if occured. Let's now go to the terminal and run the application.

```
node index.js
<!-- Listening on port 3005 -->
```

It means that we have successfully connected our application to the database.

### Adding methods to the pool

Let's first add a `close` function to the pool class. This method will be able to close out the pool or essentially disconnect from the Postgres database.

```js
class Pool {
  _pool = null;

  connect(options) {
    this._pool = new pg.Pool(options);
    return this._pool.query("SELECT 1 + 1;");
  }

  close() {
    return this._pool.end();
  }
}
```

We are then going to add a method called `query` to actually run some query or send some SQL to the databse, but this is going to expose a really big security issue. This implementation is not good enough and should be completed. We will discuss this later...

```js
class Pool {
  _pool = null;

  connect(options) {
    this._pool = new pg.Pool(options);
    return this._pool.query("SELECT 1 + 1;");
  }

  close() {
    return this._pool.end();
  }

  // REALLY BIG SECURITY ISSUE HERE!
  query(sql) {
    return this._pool.query(sql);
  }
}
```

# Data access patterns

Let's now implement our route handlers. For instance, If we receive a `GET` request at the `/users` route, we want to query for all the users, and return the results to the client that has requested. To run this query, we are going to use the **Repository pattern**.

Let's first understand what a repository is. What we are going to do is to create a **User repository** and define some functions for it:

![sql-indexes-20](/images/sql/sql-indexes-20.jpg)

This user repository is going to be our central access point to perform different things on our users table. Each HTTP method will be used by one of the functions of this repository.

![sql-indexes-21](/images/sql/sql-indexes-21.jpg)

Note that you are not limited to these functions for the repository, and there should not necessarily be a 1-1 connection between HTTP methods and repository functions. You could, for instance, define a `fineOne` function that could also use the `GET` HTTP method.

How can we implement the repository? There are a huge variety of approaches. You can define it as a plain object with some functiones defined on it. You can make it an instance of a class. You can make it a class with some static methods tied to it. You can do it in any way you want. It just has to be the central access point to a specific aspect of our database.

## Creating a repository

Create a file called `user-repo.js` inside a `repos` directory that is placed inside the `src` directory. Wihtin this file, you are going to create the repo.

### Creating a plain object

```js
// user-repo.js

const pool = require("../pool");

module.exports = {
  find() {},

  findById() {},

  insert() {},
};
```

### Creating a class

```js
// user-repo.js
const pool = require("../pool");

class UserRepo {
  find() {}

  findById() {}

  insert() {}
}

module.exports = new UserRepo();
```

### Creating a class with static methods

```js
// user-repo.js
const pool = require("../pool");

class UserRepo {
  static find() {}

  static findById() {}

  static insert() {}
}

module.exports = UserRepo;
```

Then you could access the functions in another file like this:

```js
UserRepo.insert();
```

We are actually going to use this method. So let's continue implementing the class. Some of the functions are going to accept some argument, but all of them are similar in nature: They are all going to use the pool to run some query against the database, and then return the results.

Running a query using the pool is an asynchronous operation. So in all the repository functions we are going to use the `async`/`await` syntax.

```js
class UserRepo {
  static async find() {}

  static async findById() {}

  static async insert() {}

  static async update() {}

  static async delete() {}
}
```

Now let's implement the functions themselves:

```js
class UserRepo {
  static async find() {
    const { rows } = await pool.query("SELECT * FROM users;");
    return rows;
  }

  // There are a little bit of holes around the logic. We cannot just return the rows. We need to do some further processing.

  static async findById() {}

  static async insert() {}

  static async update() {}

  static async delete() {}
}
```

We can now use this function in our router file.

```js
// users.js
const express = require("express");
const UserRepo = require("../repos/user-repo");
const router = express.Router();

router.get("/users", async (req, res) => {
  const users = await UserRepo.find();
  res.send(users);
});
```

## Testing the API and database connection

You can run your server application in a terminal, and then, using Postman, you can send a GET request to `localhost:3005/users` and receive all the users back from the database.

There is only one issue here. Naming columns in Postgres with underscores is a convention. So you have `created_at` in the data coming back from the database. However, if you are writing a Node application, this kind of naming does not comply with JavaScript variable naming conventions. The best way to go in these cases is to do some further processing in the repository functions; that is, process and format names before you return them as the final data.

```js
class UserRepo {
  static async find() {
    const { rows } = await pool.query("SELECT * FROM users;");

    const parsedRows = rows.map((row) => {
      const replaced = {};

      for (let key in row) {
        const camelCase = key.replace(/([-_][a-z])/gi, ($1) =>
          $1.toUpperCase().replace("_", "")
        );
        replaced[camelCase] = row[key];
      }

      return replaced;
    });

    return parsedRows;
  }
}
```

Let's refactor this cammel case processing into its own file and make it more reusable accross the application:

```js
// repo/to-camel-case.js
module.exports = (rows) => {
  return rows.map((row) => {
    const replaced = {};

    for (let key in row) {
      const camelCase = key.replace(/([-_][a-z])/gi, ($1) =>
        $1.toUpperCase().replace("_", "")
      );
      replaced[camelCase] = row[key];
    }

    return replaced;
  });
};
```

Then we can use it in the `user-repo.js` file:

```js
// user-repo.js
class UserRepo {
  static async find() {
    const { rows } = await pool.query("SELECT * FROM users;");

    return toCamelCase(rows);
  }
}
```

## Finding particular rows (users)

We are now going to implement the route handler related to finding a specific user by their ID. You can go on and implement the route handler first. You can access the ID parameter inserted into the route URL by using the `params` property of the request object.

Remember that whenever someone tries to find a specific record by its ID, they might insert an ID that does not exist on our database. So you should make sure you return a specific record to the sender of the request, and if there is no record related to what they are requesting for, you should respond with a `404` error.

```js
// user.js
router.get("/users/:id", async (req, res) => {
  const { id } = req.params;

  const user = await UserRepo.findById(id);

  if (user) {
    res.send(user);
  } else {
    res.sendStatus(404);
  }
});
```

You now need to implement the repository function `findById`. Remember that this initial implementation has a really big security issue to it. We are going to fix it later.

```js
// user-repo.js
class UserRepo {
  static async findById(id) {
    // REALLY BIG SECURITY ISSUE!
    const { rows } = await pool.query(`
        SELECT * FROM users WHERE id = ${id}
      `);

    return toCamelCase(rows)[0];
  }
}
```

You can now send a GET request to the `localhost:3005/users/:id` route and check the response.

```
localhost:3005/users/1
```

# Security around Postgres

## SQL injection exploits

We have been talking about a serious security issue. What is that? Let's send the previous GET request to the same URL but with some alteration:

```
localhost:3005/users/1;DROP TABLE users;
```

This will return an error, but it has actually destroyed the whole users table. The table no longer exists in our database. It is totally gone. This is called **SQL injection exploit**.

You should **NEVER EVER** directly concatenate user-provided input into a SQL query. There are a variety of safe ways to get user-provided values into a string.

## Handle SQL injection with prepared statements

In order to get user values into queries you need to add some code to **sanitize** user-provided values. You can also rely on Postgres to sanitize values for you.

Let's use Postgres to handle this issue. Currently, our code is implemented in a way that a query statement is created by ourselves in the code, it is simply passed to the `pg` module, and the module just executes the query at the database. We need to change this flow a little bit. We are going to split this process into two steps. We are going to give `pg` 2 separate things:

1. A query statement which is a template. So instead of this:

```sql
SELECT * FROM users
WHERE id = 127;
```

we are going to give it this:

```sql
SELECT * FROM users
WHERE id = $1
```

`$1` is a kind of an identifier that says that a value should be taken from somewhere and inserted here.

2. An array of values to be substituted into the query statement template: `['127']`

If the query statment expects two arguments in `$1` and `$2` placeholders, you would have to provide 2 values in the value array. So the first element of the array would be replaced with `$1` and the second value in the array would be replaced by `$2`.

But how is this actually used by `pg` and Postgres? The statement template is used by `pg` first. This module will create a **Prepared statement**. It will tell the database that it should get ready because we are about to execute a query like this, but we are not running it yet:

```
PREPARE sdlkfjh (string) AS
  SELECT *
  FROM users
  WHERE id = $1;
```

Note that `sdlkfjh` is a unique identifier for the prepared statement. It is randomly generated by the `pg` module. The `(string)` means that the statement will receive a string as its single argument (`$1`). It could also be of type integer.

Immediately afterwards, `pg` will try to execute the query:

```sql
EXECUTE sdlkfjh('127');
```

But how is this going to solve the SQL injection problem? Postgres is totally aware why you are using prepared statements. So it understands that the `EXECUTE` command should not pass any query. It should only pass the specified values. Let's implement this approach in our code:

```js
class UserRepo {
  static async findById(id) {
    const { rows } = await pool.query(
      `
        SELECT * FROM users WHERE id = $1;
      `,
      [id]
    );

    return toCamelCase(rows)[0];
  }
}
```

Remember that the `poo.query` function is what we defined in our `pool.js` file. So we need to update it to be able to receive 2 arguments as we updated the code above:

```js
// pool.js
class Pool {
  _pool = null;

  connect(options) {
    this._pool = new pg.Pool(options);
    return this._pool.query("SELECT 1 + 1;");
  }

  close() {
    return this._pool.end();
  }

  // SECURITY ISSUE SOLVED!
  query(sql, params) {
    return this._pool.query(sql, params);
  }
}
```

Now if you try to inject SQL into the query, database will respond with error, saying "invalid input syntax for type integer: "1;DROP TABLE users;".

The downside to this approach is that you might very well need to accept other inputs from the client that targets some other place of the query. Not always clients should provide ID values for the query. For instance, they might want to pass identifiers for the `SELECT` statement. Prepared statements cannot be used for these cases. So let's see how we could use our first option to prevent SQL injection by sanitizing user input in our code.

## Reminder on POST requests

We are now going to implement the `post` route handler to be able to create users from our API. The first thing to do here is to just make sure that we can receive some kind of data inside a POST request. You have access to the user input through `req.body`; that is the `body` property of the request object.

So let's see what we get in the API by updating the post route handler:

```js
// user.js
router.post("/users", async (req, res) => {
  console.log(req.body);

  res.send("");
});
```

Now by sending a POST request in Postman, and attaching some data in the `body` of `raw` format, you will see the data coming to your API in the terminal. Let's now implement the user insertion logic. Remember that whenever a user is created, we generally want to take the user that has been created and return it to the person who has sent the request. So the route handler should look like this for now:

```js
router.post("/users", async (req, res) => {
  const { username, bio } = req.body;

  const user = await UserRepo.insert(username, bio);

  res.send(user);
});
```

Let's implement the `UserRepo.insert` function accordingly. Note that, by default, when you send an `INSERT INTO` query to the database, you don't get back the record that was created in the table. In order to receive this newly created record, you would have to add `RETURNING *` at the end of the query:

```js
// user-repo.js
class UserRepo {
  static async find() {
    const { rows } = await pool.query("SELECT * FROM users;");

    return toCamelCase(rows);
  }

  static async findById(id) {
    const { rows } = await pool.query(
      `
        SELECT * FROM users WHERE id = $1;
      `,
      [id]
    );

    return toCamelCase(rows)[0];
  }

  static async insert(username, bio) {
    const { rows } = await pool.query(
      "INSERT INTO users (username, bio) VALUES ($1, $2) RETURNING *;",
      [username, bio]
    );

    return toCamelCase(rows)[0];
  }
}
```

You can now test the API and database functionality using Postman by sending a POST request and attaching the data in the `body` section with the `raw` format.

## Handling updates

To implement the updating logic, you should keep in mind that we are going to get 3 values from the user:

1. The ID of the user that is going to be updated: You should first check this ID to see if the users actually exists.
2. Username: This will be accessed through request body.
3. Bio: This will be accessed through request body.

```js
// user-repo.js
class UserRepo {
  static async update(id, username, bio) {
    const { rows } = await pool.query(
      "UPDATE users SET username = $1, bio = $2 WHERE id = $3 RETURNING *;",
      [username, bio, id]
    );

    return toCamelCase(rows)[0];
  }
}
```

Then define the related route handler:

```js
// users.js
router.put("/users/:id", async (req, res) => {
  const { id } = req.params;
  const { username, bio } = req.body;

  const user = await UserRepo.update(id, username, bio);

  // If the user ID does not exist, the update won't be done and the 'user' will be undefined. If there is a user, we would return it. Otherwise, we would send out a 404 error response.
  if (user) {
    res.send(user);
  } else {
    res.sendStatus(404);
  }
});
```

## Deleting users

The code to delete a user is almost identical in nature to the `put` route handler. Again, you should check if a wrong user ID is sent by the client.

```js
router.delete("/users/:id", async (req, res) => {
  const { id } = req.params;

  const user = await UserRepo.delete(id);

  if (user) {
    res.send(user);
  } else {
    res.sendStatus(404);
  }
});
```

Let's now implement the `delete` method of `UserRepo`.

```js
class UserRepo {
  static async delete(id) {
    const { rows } = await pool.query(
      "DELETE FROM users WHERE id = $1 RETURNING *;",
      [id]
    );

    return toCamelCase(rows)[0];
  }
}
```

It is now time to implement some API testing system. Note that doing this requires a deeper understanding of how you are connecting to the database and other stuff around it.

# Fast parallel testing

The main challenge in impelmenting tests using the Jest library is that Jest, unlike many other libraries, run multiple test files at the same time. Therefore, there might be scenarios when these test files get stuck in conflicts with each other's process. For instance, one test file might carry on the process of creating a user and then right after, getting the recently created user. This might very well get into conflict with another test file which is creating a user for another purpose. In order to solve this issue, we need to acquire a bit deeper understanding of Postgres internals. Beware that this is going to be more about Postgres rather than the testing strategies and practices.

## Assertions around user count

Let's start by writing some test. Inside the `src` directory, we are going to build another directory called `test`, and another directory nested under `test`, called `routes`, and in this final directory, we are going to create a file called `users.test.js`.

First we are going to require the `supertest` module. It is a module that we installed its NPM package at the very beginning of our project. Then we are going to require the app that we created by calling express from the `app.js` file.

```js
// users.test.js
const request = require("supertest");
const buildApp = require("../../app");
```

You can write the test using this code:

```js
// users.test.js
const request = require("supertest");
const buildApp = require("../../app");

it("create a user", async () => {
  await request(buildApp())
    .post("/users")
    .send({ username: "testuser", bio: "test bio" })
    .expect(200);
});
```

But to really test for the actual data at the database level, we would have to use the `UserRepo` class to reach into the database. A common pattern regarding this matter is to count the number of users before the test, and compare it with the number of users after the test. So we are now going to need to implement a functionality for the `UserRepo` to get the user count from the database.

```js
// user-repo.js
class UserRepo {
  static async count() {
    const { rows } = await pool.query("SELECT COUNT(*) FROM users;");

    return rows[0].count;
  }
}
```

We are then going to require the `UserRepo` module into the test file and use it in the testing function:

```js
// users.test.js
const request = require("supertest");
const buildApp = require("../../app");
const UserRepo = require("../../repos/user-repo");

it("create a user", async () => {
  const startingCount = await UserRepo.count();
  expect(startingCount).toEqual(0);

  await request(buildApp())
    .post("/users")
    .send({ username: "testuser", bio: "test bio" })
    .expect(200);

  const finishCount = await UserRepo.count();
  expect(finishCount).toEqual(1);
});
```

## Connecting to database for tests

Now as we run this, we are going to see a whole barrel of different errors. To run the test, we are first going to open up our `package.json` file, and add a `test` script:

```json
{
  "scripts": {
    "migrate": "node-pg-migrate",
    "start": "nodemon index.js",
    "test": "jest"
  }
}
```

Then move to the terminal, stop the running server, and execute the test:

```
npm run test
```

This will return an error:

```
TypeError: Cannot read properties of null (reading 'query')
at Pool.query (src/pool.js:16:23)
```

Let's go to the file. The error is coming from the `query` function in the pool. It seems that `this._pool` is being evaluated to `null`. Why?

```js
class Pool {
  _pool = null;

  connect(options) {
    this._pool = new pg.Pool(options);
    return this._pool.query("SELECT 1 + 1;");
  }

  close() {
    return this._pool.end();
  }

  query(sql, params) {
    return this._pool.query(sql, params);
  }
}
```

Remember that the `query` function can only get called after the `connect` function actually connects the application to the database. Our application starts to listen for requests only if the connection is established. When running the test, we are not telling it run the connection first. To implement this for the testing environment, we can use the `beforeAll` function at the beginning of the test file:

```js
// users.test.js
const request = require("supertest");
const buildApp = require("../../app");
const UserRepo = require("../../repos/user-repo");
const pool = require("../../pool");

beforeAll(() => {
  return pool.connect({
    host: "localhost",
    port: 5432,
    database: "socialnetwork",
    user: "postgres",
    password: "hotButter",
  });
});
// pool.connect return a promise that resolves after connection to the database is successfully established. Jest understands the promise and waits for it to get resolved, and then runs the test.

it("create a user", async () => {
  const startingCount = await UserRepo.count();
  expect(startingCount).toEqual(0);

  await request(buildApp())
    .post("/users")
    .send({ username: "testuser", bio: "test bio" })
    .expect(200);

  const finishCount = await UserRepo.count();
  expect(finishCount).toEqual(1);
});
```

Let's run the test again from the terminal. But there is another error:

```
expect(received).toEqual(expected) // deep equality

    Expected: 0
    Received: "1"
```

So the initial expectation did not work as expected. This is one problem. There is also another problem:

```
Jest did not exit one second after the test run has completed.
```

It means that after the test process was completed, our code did not quit as it should, and kept running.

## Disconnecting after tests

Let's fix the second problem first. This problem might seem a small thing, but it turnes out that stuff like this starts to get really important if you ever start to wire up your test suite to a CI pipeline or essentially some kind of pipeline that is going to automatically run your tests.

So we created a connection at the beginning of the test file. By establishing this connection, we are actually asking `pg` to hold on to this connection unless told otherwise. We are going to use the `close` method that we define inside the `Pool` class. We can use this method in a `afterAll` function in the test file. This function will only run after the test process is complete.

```js
// users.test.js
beforeAll(() => {
  return pool.connect({
    host: "localhost",
    port: 5432,
    database: "socialnetwork",
    user: "postgres",
    password: "hotButter",
  });
});

afterAll(() => {
  return pool.close();
});
```

Now if you run the test again, you will only receive the first error related to the initial expectations.

## Multi-database setup and fixing some errors

What we are doing with our application right now is to run it in two modes: development and testing. Currently, in both modes, our application connects to the same database.

The best solution for this is to create a second database and make the application to connect to this second database whenever you want to run tests. So let's create a new table in PGAdmin and call it `socialnetwork0-test` and make our application connect to this database when testing. To do this we would have to update the `beforeAll` function like this:

```js
beforeAll(() => {
  return pool.connect({
    host: "localhost",
    port: 5432,
    database: "socialnetwork-test",
    user: "postgres",
    password: "hotButter",
  });
});

afterAll(() => {
  return pool.close();
});

it("create a user", async () => {
  const startingCount = await UserRepo.count();
  expect(startingCount).toEqual(0);

  await request(buildApp())
    .post("/users")
    .send({ username: "testuser", bio: "test bio" })
    .expect(200);

  const finishCount = await UserRepo.count();
  expect(finishCount).toEqual(1);
});
```

But as you might guess, this test will still fail because in this new database we don't have any table called `users`. We just created a new database, we neither did run any migrations on it, nor creating the table manually inside PGAdmin. So let's go to the terminal and run the migration that we had previously set up:

```
DATABASE_URL=postgres://postgres:hotButter@localhost:5432/socialnetwork-test npm run migrate up
```

> note that we have updated the database name to `socialnetwork-test`.

Let's now finally run the test again:

```
npm run test
```

Another error! Jest expected 0 as number, but it received 0 as string. Let's fix this:

```js
// user-repo.js
class UserRepo {
  static async count() {
    const { rows } = await pool.query("SELECT COUNT(*) FROM users;");

    return parseInt(rows[0].count);
  }
}
```

Run the test again and you'll succeed. Now try to run the test again right after. Again you will fail! This is obvious. Your test database is now failing Jest's initial expectation. The test stil expects 0 but now, after that first successful test, is receives 1.

There are a couple of different things we can do for this. You could update your test logic as:

```js
it("create a user", async () => {
  const startingCount = await UserRepo.count();

  await request(buildApp())
    .post("/users")
    .send({ username: "testuser", bio: "test bio" })
    .expect(200);

  const finishCount = await UserRepo.count();
  expect(finishCount - startingCount).toEqual(1);
});
```

This test setup will allow us to run the test against any state of the database table. This will make our test run as much as we want and will not fail. But this fix, is not goint to work all the times.

Let's go on and create another test file and see what issues we would have with **parallel tests**.

## Issues with parallel tests

We are now going to duplicate our test file twice, and run all the test files at the same time. To be able to run simultaneous tests, you first need to update your scripts in the `package.json` file.

```json
{
  "scripts": {
    "migrate": "node-pg-migrate",
    "start": "nodemon index.js",
    "test": "jest --no-cache"
  }
}
```

The `--no-cache` flag will force Jest to run all our test files in parallel. Let's now duplicate our test files with names `users-two.test.js` and `users-three.test.js`. Let's now go to the terminal and run our tests:

```
npm run test
```

They will probably all fail. Maybe not all of them but some of them will definitely fail. It is kind of random what you'll get. Why is that? So each test is going through this process:

1. get count
2. create user
3. get new count
4. new - old === 1

Running 3 tests simultaneously will make at least some of the test files fail their final expectiation. For some or all of them `new - old` might not become `1` anymore. It might be more or less than 1 depending on the execution speed.

What we should do essentially, is to make each test file run in its own little isolated environment.

## Isolation with schemas

Currently, all the three test files are trying to access the same table inside the test database. This is the origin of conflicts in the parallel tests we just executed.

The first possible solution is to make each test file gets its own database. The upside to this approach is that each test file can apply as many changes as they are inteded to with no limit. The downside, however, is that you would have to create many different databases as the number of your test files increase.

Another solution is to use a little bit more complex mechanism inside Postgres. We are going to make sure each test file gets its own **Schema**. A schema is like a folder to organize things in a database. Every database gets a default schema called `public`. Each schema can have its own separate copy of a table. They can have any object in them, including tables.

So as for the testing files, we can tell each of them to connect to a different schema and use the table inside it. So we are going to use different schemas inside one single database.

### How schemas work behind the scenes

Let's run some commands inside PGAdmin to create schemas and then create a table or two inside the schemas. What we are going to do here is to create a schema called `test` and create a copy of the users table inside it.

So in the query tool of our `socialnetwork-test` database:

```sql
CREATE SCHEMA test;
```

Let's now create a users table inside this new schema. To create a table inside a specific schema, rather than the default `public` schema, there should be a slight difference in the command. You should write the schema name before the table name.

```sql
CREATE TABLE test.users(
	id SERIAL PRIMARY KEY,
	username VARCHAR
);
```

We can now add some user records to this table:

```sql
INSERT INTO test.users (username)
VALUES
	('alex'),
	('lajkdhflaskj');
```

In order to retrieve user records from this schema we would have to use this command:

```sql
SELECT * FROM test.users;
```

So if you ignore typing the schema name before the table's name, Postgres will, by default, access the `public` schema. You can change this default behavior. To do so, you can use an internal Postgres mechanism called **Search path**. Search path controls which schema is Postgres going to access by default, if you do not specifically mention the schema name.

In order to see the value of search path in PGAdmin, you can use this command:

```sql
SHOW search_path;
-- returns search_path = "$user", public
```

This means that Postgres will, by default, try to access the public schema if no schema name is mentioned in the query.

What is the `$user` about? Whenever you connect to your Postgres instance, you are connecting as a specific user. On windows, the username of that specific user is `postgres`. This is the same username that you pass to the `connect` function of the pool. So `"$user", public` in the returned result of the `SHOW` command means that whenever you run a query, Postgres will first try to access a schema with a name that is the same as the username. If such schema is not found, Postgres will try to access the `public` schema.
